{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression data using scikit-learn\n",
    "\n",
    "Source: <https://github.com/IBM/ml-learning-path-assets/tree/master/notebooks>\n",
    "\n",
    "Regression is when the feature to be predicted contains continuous values. Regression refers to the process of predicting a dependent variable by analyzing the relationship between other independent variables. There are several algorithms known to us that help us in excavating these relationships to better predict the value.\n",
    "\n",
    "In this notebook, we'll use scikit-learn to predict values. Scikit-learn provides implementations of many regression algorithms. In here, we have done a comparative study of 5 different regression algorithms. \n",
    "\n",
    "To help visualize what we are doing, we'll use 2D and 3D charts to show how the classes looks (with 3 selected dimensions) with matplotlib and seaborn python libraries.\n",
    "\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Table of Contents\n",
    "\n",
    "1. [Load libraries](#load_libraries)\n",
    "2. [Helper methods for metrics](#helper_methods)\n",
    "3. [Data exploration](#explore_data)\n",
    "4. [Prepare data for building regression model](#prepare_data)\n",
    "5. [Build Simple Linear Regression model](#model_slr)\n",
    "6. [Build Multiple Linear Regression classification model](#model_mlr)\n",
    "7. [Build Polynomial Linear Regression model](#model_plr) \n",
    "8. [Build Decision Tree Regression model](#model_dtr) \n",
    "9. [Build Random Forest Regression model](#model_rfr)\n",
    "10. [Comparitive study of different regression algorithms](#compare_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick set of instructions to work through the notebook\n",
    "\n",
    "If you are new to Notebooks, here's a quick overview of how to work in this environment.\n",
    "\n",
    "1. The notebook has 2 types of cells - markdown (text) such as this and code such as the one below. \n",
    "2. Each cell with code can be executed independently or together (see options under the Cell menu). When working in this notebook, we will be running one cell at a time because we need to make code changes to some of the cells.\n",
    "3. To run the cell, position cursor in the code cell and click the Run (arrow) icon. The cell is running when you see the * next to it. Some cells have printable output.\n",
    "4. Work through this notebook by reading the instructions and executing code cell by cell. Some cells will require modifications before you run them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load_libraries\"></a>\n",
    "## 1. Load libraries\n",
    "[Top](#top)\n",
    "\n",
    "Install python modules\n",
    "NOTE! Some pip installs require a kernel restart.\n",
    "The shell command pip install is used to install Python modules. Some installs require a kernel restart to complete. To avoid confusing errors, run the following cell once and then use the Kernel menu to restart the kernel before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,mean_squared_error, r2_score\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "import sys\n",
    "import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"helper_methods\"></a>\n",
    "## 2. Helper methods for metrics\n",
    "[Top](#top)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def two_d_compare(X_test,y_test,y_pred,model_name):\n",
    "    area = (12 * np.random.rand(40))**2 \n",
    "    plt.subplots(ncols=2, figsize=(10,4))\n",
    "    plt.suptitle('Actual vs Predicted data : ' +model_name + '. Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(X_test, y_test, alpha=0.8, color='#8CCB9B')\n",
    "    plt.title('Actual')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.scatter(X_test, y_pred,alpha=0.8, color='#E5E88B')\n",
    "    plt.title('Predicted')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def model_metrics(regressor,y_test,y_pred):\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "      % mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print('R2 score: %.2f' % r2 )\n",
    "    return [mse, r2]\n",
    "\n",
    "def two_vs_three(x_test,y_test,y_pred,z=None, isLinear = False) : \n",
    "    \n",
    "    area = 60\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    fig.suptitle('2D and 3D view of sales price data')\n",
    "\n",
    "    # First subplot\n",
    "    ax = fig.add_subplot(1, 2,1)\n",
    "    ax.scatter(x_test, y_test, alpha=0.5,color='blue', s= area)\n",
    "    ax.plot(x_test, y_pred, alpha=0.9,color='red', linewidth=2)\n",
    "    ax.set_xlabel('YEAR BUILT')\n",
    "    ax.set_ylabel('SELLING PRICE')\n",
    "    \n",
    "    plt.title('YEARBUILT vs SALEPRICE')\n",
    "    \n",
    "    if not isLinear : \n",
    "    # Second subplot\n",
    "        ax = fig.add_subplot(1,2,2, projection='3d')\n",
    "\n",
    "        ax.scatter(z, x_test, y_test, color='blue', marker='o')\n",
    "        ax.plot(z, x_test, y_pred, alpha=0.9,color='red', linewidth=2)\n",
    "        ax.set_ylabel('YEAR BUILT')\n",
    "        ax.set_zlabel('SELLING PRICE')\n",
    "        ax.set_xlabel('LOT AREA')\n",
    "\n",
    "    plt.title('LOT AREA vs YEAR BUILT vs SELLING PRICE')\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"explore_data\"></a>\n",
    "## 3. Data exploration\n",
    "[Top](#top)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the snippet below, we use the pandas library to load a csv that contains housing related information. With several independent variables related to this domain, we are going to predict the sales price of a house. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data from <https://raw.githubusercontent.com/IBM/ml-learning-path-assets/master/data/predict_home_value.csv>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd =  pd.read_csv(\"/data/IFI8410/sess09/predict_home_value.csv\")\n",
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = 60\n",
    "x = df_pd['YEARBUILT']\n",
    "y = df_pd['SALEPRICE']\n",
    "z = df_pd['LOTAREA']\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "fig.suptitle('2D and 3D view of sales price data')\n",
    "\n",
    "# First subplot\n",
    "ax = fig.add_subplot(1, 2,1)\n",
    "\n",
    "ax.scatter(x, y, alpha=0.5,color='blue', s= area)\n",
    "ax.set_xlabel('YEAR BUILT')\n",
    "ax.set_ylabel('SELLING PRICE')\n",
    "\n",
    "plt.title('YEARBUILT vs SALEPRICE')\n",
    "\n",
    "# Second subplot\n",
    "ax = fig.add_subplot(1,2,2, projection='3d')\n",
    "\n",
    "ax.scatter(z, x, y, color='blue', marker='o')\n",
    "\n",
    "ax.set_ylabel('YEAR BUILT')\n",
    "ax.set_zlabel('SELLING PRICE')\n",
    "ax.set_xlabel('LOT AREA')\n",
    "\n",
    "plt.title('LOT AREA VS YEAR BUILT vs SELLING PRICE')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\": (8, 4)}); np.random.seed(0)\n",
    "# ax = sns.distplot(df_pd['SALEPRICE'])\n",
    "ax = sns.displot(df_pd['SALEPRICE'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"The dataset contains columns of the following data types : \\n\" +str(df_pd.dtypes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice below that FIREPLACEQU, GARAGETYPE, GARAGEFINISH, GARAGECOND,FENCE and POOLQC have missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The dataset contains following number of records for each of the columns : \\n\" +str(df_pd.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd.isnull().any()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prepare_data\"></a>\n",
    "## 4. Data preparation\n",
    "[Top](#top)\n",
    "\n",
    "Data preparation is a very important step in machine learning model building. This is because the model can perform well only when the data it is trained on is good and well prepared. Hence, this step consumes bulk of data scientist's time spent building models.\n",
    "\n",
    "During this process, we identify categorical columns in the dataset. Categories needed to be indexed, which means the string labels are converted to label indices. These label indices are encoded using One-hot encoding to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features to use categorical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#remove columns that are not required\n",
    "df_pd = df_pd.drop(['ID'], axis=1)\n",
    "\n",
    "df_pd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the categorical columns \n",
    "categoricalColumns = df_pd.select_dtypes(include=[object]).columns\n",
    "\n",
    "print(\"Categorical columns : \" )\n",
    "print(categoricalColumns)\n",
    "\n",
    "impute_categorical = SimpleImputer(strategy=\"most_frequent\")\n",
    "onehot_categorical =  OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[('impute',impute_categorical),('onehot',onehot_categorical)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the numerical columns \n",
    "numericalColumns = [col for col in df_pd.select_dtypes(include=[float,int]).columns if col not in ['SALEPRICE']]\n",
    "print(\"Numerical columns : \" )\n",
    "print(numericalColumns)\n",
    "\n",
    "scaler_numerical = StandardScaler()\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[('scale',scaler_numerical)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessorForCategoricalColumns = ColumnTransformer(transformers=[('cat', categorical_transformer, categoricalColumns)],\n",
    "                                            remainder=\"passthrough\")\n",
    "preprocessorForAllColumns = ColumnTransformer(transformers=[('cat', categorical_transformer, categoricalColumns),('num',numerical_transformer,numericalColumns)],\n",
    "                                            remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "#. The transformation happens in the pipeline. Temporarily done here to show what intermediate value looks like\n",
    "df_pd_temp = preprocessorForCategoricalColumns.fit_transform(df_pd)\n",
    "print(\"Data after transforming :\")\n",
    "print(df_pd_temp)\n",
    "\n",
    "df_pd_temp_2 = preprocessorForAllColumns.fit_transform(df_pd)\n",
    "print(\"Data after transforming :\")\n",
    "print(df_pd_temp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare data frame for splitting data into train and test datasets\n",
    "\n",
    "features = []\n",
    "features = df_pd.drop(['SALEPRICE'], axis=1)\n",
    "\n",
    "label = pd.DataFrame(df_pd, columns = ['SALEPRICE']) \n",
    "#label_encoder = LabelEncoder()\n",
    "label = df_pd['SALEPRICE']\n",
    "\n",
    "#label = label_encoder.fit_transform(label)\n",
    "print(\" value of label : \" + str(label))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_slr\"></a>\n",
    "## 5. Simple linear regression\n",
    "[Top](#top)\n",
    "\n",
    "This is the most basic form of linear regression in which the variable to be predicted is dependent on only one other variable. This is calculated by using the formula that is generally used in calculating the slope of a line.\n",
    "\n",
    "y = w0 + w1*x1\n",
    "\n",
    "In the above equation, y refers to the target variable and x1 refers to the independent variable. w1 refers to the coeeficient that expresses the relationship between y and x1 is it also know as the slope. w0 is the constant cooefficient a.k.a the intercept. It refers to the constant offset that y will always be with respect to the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since simple linear regression assumes that output depends on only one variable, we are assuming that it depends on the YEARBUILT. Data is split up into training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = features['YEARBUILT'].values.reshape(-1,1)\n",
    "X_train_slr, X_test_slr, y_train_slr, y_test_slr = train_test_split(X,label , random_state=0)\n",
    "\n",
    "print(\"Dimensions of datasets that will be used for training : Input features\"+str(X_train_slr.shape)+ \n",
    "      \" Output label\" + str(y_train_slr.shape))\n",
    "print(\"Dimensions of datasets that will be used for testing : Input features\"+str(X_test_slr.shape)+ \n",
    "      \" Output label\" + str(y_test_slr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_name = 'Simple Linear Regression'\n",
    "\n",
    "slRegressor = LinearRegression()\n",
    "\n",
    "slRegressor.fit(X_train_slr,y_train_slr)\n",
    "\n",
    "y_pred_slr= slRegressor.predict(X_test_slr)\n",
    "\n",
    "print(slRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intercept: \\n',slRegressor.intercept_)\n",
    "print('Coefficients: \\n', slRegressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_vs_three(X_test_slr[:,0],y_test_slr,y_pred_slr,None, True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d_compare(X_test_slr,y_test_slr,y_pred_slr,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slrMetrics = model_metrics(slRegressor,y_test_slr,y_pred_slr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_lrc\"></a>\n",
    "## 6. Build multiple linear regression model\n",
    "[Top](#top)\n",
    "\n",
    "Multiple linear regression is an extension to the simple linear regression. In this setup, the target value is dependant on more than one variable. The number of variables depends on the use case at hand. Usually a subject matter expert is involved in identifying the fields that will contribute towards better predicting the output feature.\n",
    "\n",
    "y = w0 + w1*x1 + w2*x2 + .... + wn*xn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since multiple linear regression assumes that output depends on more than one variable, we are assuming that it depends on all the 30 features. Data is split up into training and test sets. As an experiment, you can try to remove a few features and check if the model performs any better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features,label , random_state=0)\n",
    "\n",
    "print(\"Dimensions of datasets that will be used for training : Input features\"+str(X_train.shape)+ \n",
    "      \" Output label\" + str(y_train.shape))\n",
    "print(\"Dimensions of datasets that will be used for testing : Input features\"+str(X_test.shape)+ \n",
    "      \" Output label\" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_name = 'Multiple Linear Regression'\n",
    "\n",
    "mlRegressor = LinearRegression()\n",
    "\n",
    "mlr_model = Pipeline(steps=[('preprocessorAll',preprocessorForAllColumns),('regressor', mlRegressor)])\n",
    "\n",
    "mlr_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_mlr= mlr_model.predict(X_test)\n",
    "\n",
    "print(mlRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intercept: \\n',mlRegressor.intercept_)\n",
    "print('Coefficients: \\n', mlRegressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_vs_three(X_test['YEARBUILT'],y_test,y_pred_mlr,X_test['LOTAREA'], False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d_compare(X_test['YEARBUILT'],y_test,y_pred_mlr,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlrMetrics = model_metrics(slRegressor,y_test,y_pred_mlr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_plr\"></a>\n",
    "## 7. Build Polynomial Linear regression model\n",
    "[Top](#top)\n",
    "\n",
    "The prediction line generated by simple/linear regression is usually a straight line. In cases when a simple or multiple linear regression does not fit the data point accurately, we use the polynomial linear regression. The following formula is used in the back-end to generate polynomial linear regression.\n",
    "\n",
    "y = w0 + w1*x1 + w2*x21 + .... + wn*xnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assuming that output depends on the YEARBUILT and LOTATREA. Data is split up into training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features.iloc[:, [0,4]].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,label, random_state=0)\n",
    "\n",
    "print(\"Dimensions of datasets that will be used for training : Input features\"+str(X_train.shape)+ \n",
    "      \" Output label\" + str(y_train.shape))\n",
    "print(\"Dimensions of datasets that will be used for testing : Input features\"+str(X_test.shape)+ \n",
    "      \" Output label\" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "model_name = 'Polynomial Linear Regression'\n",
    "\n",
    "polynomial_features= PolynomialFeatures(degree=3)\n",
    "plRegressor = LinearRegression()\n",
    "\n",
    "plr_model = Pipeline(steps=[('polyFeature',polynomial_features ),('regressor', plRegressor)])\n",
    "\n",
    "plr_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_plr= plr_model.predict(X_test)\n",
    "\n",
    "print(plRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intercept: \\n',plRegressor.intercept_)\n",
    "print('Coefficients: \\n', plRegressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_vs_three(X_test[:,1],y_test,y_pred_plr,X_test[:,0], False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d_compare(X_test[:,1],y_test,y_pred_plr,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plrMetrics = model_metrics(plRegressor,y_test,y_pred_plr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_dtr\"></a>\n",
    "## 8. Build decision tree regressor\n",
    "[Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,df_pd['SALEPRICE'] , random_state=0)\n",
    "\n",
    "print(\"Dimensions of datasets that will be used for training : Input features\"+str(X_train.shape)+ \n",
    "      \" Output label\" + str(y_train.shape))\n",
    "print(\"Dimensions of datasets that will be used for testing : Input features\"+str(X_test.shape)+ \n",
    "      \" Output label\" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model_name = \"Decision Tree Regressor\"\n",
    "\n",
    "decisionTreeRegressor = DecisionTreeRegressor(random_state=0,max_features=30)\n",
    "\n",
    "dtr_model = Pipeline(steps=[('preprocessorAll',preprocessorForAllColumns),('regressor', decisionTreeRegressor)]) \n",
    "\n",
    "dtr_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_dtr = dtr_model.predict(X_test)\n",
    "\n",
    "print(decisionTreeRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(decisionTreeRegressor, out_file ='tree.dot')  \n",
    "# Use dot file to create a graph\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "# Write graph to a png file\n",
    "graph.write_png('tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d_compare(X_test['YEARBUILT'],y_test,y_pred_dtr,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrMetrics = model_metrics(decisionTreeRegressor,y_test,y_pred_dtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_rfr\"></a>\n",
    "## 9. Build Random Forest classification model\n",
    "[Top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree algorithms are efficient in eliminating columns that don't add value in predicting the output and in some cases, we are even able to see how a prediction was derived by backtracking the tree. However, this algorithm doesn't perform individually when the trees are huge and are hard to interpret. Such models are oftern referred to as weak models. The model performance is however improvised by taking an average of several such decision trees derived from the subsets of the training data. This approach is called the Random Forest Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_name = \"Random Forest Regressor\"\n",
    "\n",
    "randomForestRegressor = RandomForestRegressor(n_estimators=100, max_depth=15,random_state=0)\n",
    "\n",
    "rfr_model = Pipeline(steps=[('preprocessorAll',preprocessorForAllColumns),('regressor', randomForestRegressor)]) \n",
    "\n",
    "rfr_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_rfr = rfr_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d_compare(X_test['YEARBUILT'],y_test,y_pred_rfr,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfrMetrics = model_metrics(randomForestRegressor,y_test,y_pred_rfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"compare_classification\"></a>\n",
    "## 10. Comparative study of different regression algorithms. \n",
    "[Top](#top)\n",
    "\n",
    "In the bar chart below, we have compared the performances of different regression algorithms with each other. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_groups = 1\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 1\n",
    "opacity = 0.8\n",
    "\n",
    "\n",
    "area = 60 \n",
    "plt.subplots(ncols=2, figsize=(12,9))\n",
    "plt.suptitle('Model performance comparison')\n",
    "\n",
    "plt.subplot(121)\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 1\n",
    "opacity = 0.8\n",
    "\n",
    "rects1 = plt.bar(index, slrMetrics[0], bar_width,\n",
    "alpha=opacity,\n",
    "color='g',\n",
    "label='Simple Linear Regression')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, mlrMetrics[0], bar_width,\n",
    "alpha=opacity,\n",
    "color='pink',\n",
    "label='Multiple Linear Regression')\n",
    "\n",
    "rects3 = plt.bar(index + bar_width*2, plrMetrics[0], bar_width,\n",
    "alpha=opacity,\n",
    "color='y',\n",
    "label='Polynomial Linear Regression')\n",
    "\n",
    "rects4 = plt.bar(index + bar_width*3, dtrMetrics[0], bar_width,\n",
    "alpha=opacity,\n",
    "color='b',\n",
    "label='Decision Tree Regression')\n",
    "\n",
    "\n",
    "rects6 = plt.bar(index + bar_width*4, rfrMetrics[0], bar_width,\n",
    "alpha=opacity,\n",
    "color='purple',\n",
    "label='Random Forest Regression')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Mean Square Error comparison.')\n",
    "#ax.set_xticklabels(('', 'Simple Lin', 'Multiple Lin', 'Polynomial Lin', 'Decision Tree','Random Forest'))\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "rects1 = plt.bar(index, slrMetrics[1], bar_width,\n",
    "alpha=opacity,\n",
    "color='g',\n",
    "label='Simple Linear Regression')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, mlrMetrics[1], bar_width,\n",
    "alpha=opacity,\n",
    "color='pink',\n",
    "label='Multiple Linear Regression')\n",
    "\n",
    "rects3 = plt.bar(index + bar_width*2, plrMetrics[1], bar_width,\n",
    "alpha=opacity,\n",
    "color='y',\n",
    "label='Polynomial Linear Regression')\n",
    "\n",
    "rects4 = plt.bar(index + bar_width*3, dtrMetrics[1], bar_width,\n",
    "alpha=opacity,\n",
    "color='b',\n",
    "label='Decision Tree Regression')\n",
    "\n",
    "\n",
    "rects6 = plt.bar(index + bar_width*4, rfrMetrics[1], bar_width,\n",
    "alpha=opacity,\n",
    "color='purple',\n",
    "label='Random Forest Regression')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('R2')\n",
    "plt.title('R2 comparison.')\n",
    "ax.set_xticklabels(('', 'Simple Lin', 'Multiple Lin', 'Polynomial Lin', 'Decision Tree','Random Forest'))\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font size=-1 color=gray>\n",
    "&copy; Copyright 2019 IBM Corp. All Rights Reserved.\n",
    "<p>\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file\n",
    "except in compliance with the License. You may obtain a copy of the License at\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the\n",
    "License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "express or implied. See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "</font></p>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
