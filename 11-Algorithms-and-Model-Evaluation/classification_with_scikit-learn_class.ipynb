{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification data using scikit-learn\n",
    "\n",
    "Source:\n",
    "- <https://developer.ibm.com/learningpaths/learning-path-machine-learning-for-developers/learn-classification-algorithms/>\n",
    "- <https://github.com/IBM/ml-learning-path-assets/blob/master/notebooks/classification_with_scikit-learn.ipynb>\n",
    "\n",
    "\n",
    "Classification problems are those in which the feature to be predicted contains categories of values. Each of these categories are considered as a class into which the predicted value will fall into and hence has its name, classification.\n",
    "\n",
    "In this notebook, we'll use scikit-learn to predict classes. scikit-learn provides implementations of many classification algorithms. In here, we have done a comparative study of 5 different classification algorithms. \n",
    "\n",
    "To help visualize what we are doing, we'll use 2D and 3D charts to show how the classes look (with 3 selected dimensions) with matplotlib and scikitplot python libraries.\n",
    "\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Table of Contents\n",
    "\n",
    "1. [Load libraries](#load_libraries)\n",
    "2. [Data exploration](#explore_data)\n",
    "3. [Prepare data for building classification model](#prepare_data)\n",
    "4. [Split data into train and test sets](#split_data)\n",
    "5. [Helper methods for graph generation](#helper_methods)\n",
    "6. [Build Naive Bayes classification model](#model_nb)\n",
    "7. [Build Logistic Regression classification model](#model_lrc)\n",
    "8. [Build K-Nearest classification model](#model_knn) \n",
    "9. [Build Kernel SVM classification model](#model_svm) \n",
    "10. [Build Random Forest classification model](#model_rfc)\n",
    "11. [Comparative study of different classification algorithms](#compare_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick set of instructions to work through the notebook\n",
    "\n",
    "If you are new to Notebooks, here's a quick overview of how to work in this environment.\n",
    "\n",
    "1. The notebook has 2 types of cells - markdown (text) such as this and code such as the one below. \n",
    "2. Each cell with code can be executed independently or together (see options under the Cell menu). When working in this notebook, we will be running one cell at a time because we need to make code changes to some of the cells.\n",
    "3. To run the cell, position cursor in the code cell and click the Run (arrow) icon. The cell is running when you see the * next to it. Some cells have printable output.\n",
    "4. Work through this notebook by reading the instructions and executing code cell by cell. Some cells will require modifications before you run them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load_libraries\"></a>\n",
    "## 1. Load libraries\n",
    "[Top](#top)\n",
    "\n",
    "Install python modules\n",
    "NOTE! Some pip installs require a kernel restart.\n",
    "The shell command pip install is used to install Python modules. Some installs require a kernel restart to complete. To avoid confusing errors, run the following cell once and then use the Kernel menu to restart the kernel before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import scikitplot as skplt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"explore_data\"></a>\n",
    "## 2. Data exploration\n",
    "[Top](#top)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data from <https://raw.githubusercontent.com/IBM/ml-learning-path-assets/master/data/mergedcustomers_missing_values_GENDER.csv>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn_pd = pd.read_csv(\"/data/IFI8410/sess09/mergedcustomers_missing_values_GENDER.csv\")\n",
    "df_churn_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"The dataset contains columns of the following data types : \\n\" +str(df_churn_pd.dtypes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice that Gender has three missing values. This will be handled in one of the preprocessing steps that is to follow. \n",
    "print(\"The dataset contains following number of records for each of the columns : \\n\" +str(df_churn_pd.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Each category within the churnrisk column has the following count : \")\n",
    "print(df_churn_pd.groupby(['CHURNRISK']).size())\n",
    "#bar chart to show split of data\n",
    "index = ['High','Medium','Low']\n",
    "churn_plot = df_churn_pd['CHURNRISK'].value_counts(sort=True, ascending=False).plot(kind='bar',figsize=(4,4),title=\"Total number for occurences of churn risk \" + str(df_churn_pd['CHURNRISK'].count()), color=['#BB6B5A','#8CCB9B','#E5E88B'])\n",
    "churn_plot.set_xlabel(\"Churn Risk\")\n",
    "churn_plot.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prepare_data\"></a>\n",
    "## 3. Data preparation\n",
    "[Top](#top)\n",
    "\n",
    "Data preparation is a very important step in machine learning model building. This is because the model can perform well only when the data it is trained on is good and well prepared. Hence, this step consumes the bulk of a data scientist's time spent building models.\n",
    "\n",
    "During this process, we identify categorical columns in the dataset. Categories needed to be indexed, which means the string labels are converted to label indices. These label indices are encoded using One-hot encoding to a binary vector with at most a single-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features to use categorical features.\n",
    "\n",
    "Final step in the data preparation process is to assemble all the categorical and non-categorical columns into a feature vector. We use VectorAssembler for this. VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#remove columns that are not required\n",
    "df_churn_pd = df_churn_pd.drop(['ID'], axis=1)\n",
    "\n",
    "df_churn_pd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the categorical columns \n",
    "categoricalColumns = ['GENDER', 'STATUS', 'HOMEOWNER']\n",
    "\n",
    "print(\"Categorical columns : \" )\n",
    "print(categoricalColumns)\n",
    "\n",
    "impute_categorical = SimpleImputer(strategy=\"most_frequent\")\n",
    "onehot_categorical =  OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[('impute',impute_categorical),('onehot',onehot_categorical)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the numerical columns \n",
    "numericalColumns = df_churn_pd.select_dtypes(include=[float,int]).columns\n",
    "\n",
    "print(\"Numerical columns : \" )\n",
    "print(numericalColumns)\n",
    "\n",
    "scaler_numerical = StandardScaler()\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[('scale',scaler_numerical)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessorForCategoricalColumns = ColumnTransformer(transformers=[('cat', categorical_transformer, categoricalColumns)],\n",
    "                                            remainder=\"passthrough\")\n",
    "preprocessorForAllColumns = ColumnTransformer(transformers=[('cat', categorical_transformer, categoricalColumns),('num',numerical_transformer,numericalColumns)],\n",
    "                                            remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "#. The transformation happens in the pipeline. Temporarily done here to show what intermediate value looks like\n",
    "df_churn_pd_temp = preprocessorForCategoricalColumns.fit_transform(df_churn_pd)\n",
    "print(\"Data after transforming :\")\n",
    "print(df_churn_pd_temp)\n",
    "\n",
    "df_churn_pd_temp_2 = preprocessorForAllColumns.fit_transform(df_churn_pd)\n",
    "print(\"Data after transforming :\")\n",
    "print(df_churn_pd_temp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare data frame for splitting data into train and test datasets\n",
    "\n",
    "features = []\n",
    "features = df_churn_pd.drop(['CHURNRISK'], axis=1)\n",
    "\n",
    "label_churn = pd.DataFrame(df_churn_pd, columns = ['CHURNRISK']) \n",
    "label_encoder = LabelEncoder()\n",
    "label = df_churn_pd['CHURNRISK']\n",
    "\n",
    "label = label_encoder.fit_transform(label)\n",
    "print(\"Encoded value of Churnrisk after applying label encoder : \" + str(label))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = 75\n",
    "x = df_churn_pd['ESTINCOME']\n",
    "y = df_churn_pd['DAYSSINCELASTTRADE']\n",
    "z = df_churn_pd['TOTALDOLLARVALUETRADED']\n",
    "\n",
    "pop_a = mpatches.Patch(color='#BB6B5A', label='High')\n",
    "pop_b = mpatches.Patch(color='#E5E88B', label='Medium')\n",
    "pop_c = mpatches.Patch(color='#8CCB9B', label='Low')\n",
    "def colormap(risk_list):\n",
    "    cols=[]\n",
    "    for l in risk_list:\n",
    "        if l==0:\n",
    "            cols.append('#BB6B5A')\n",
    "        elif l==2:\n",
    "            cols.append('#E5E88B')\n",
    "        elif l==1:\n",
    "            cols.append('#8CCB9B')\n",
    "    return cols\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "fig.suptitle('2D and 3D view of churnrisk data')\n",
    "\n",
    "# First subplot\n",
    "ax = fig.add_subplot(1, 2,1)\n",
    "\n",
    "ax.scatter(x, y, alpha=0.8, c=colormap(label), s= area)\n",
    "ax.set_ylabel('DAYS SINCE LAST TRADE')\n",
    "ax.set_xlabel('ESTIMATED INCOME')\n",
    "\n",
    "plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "\n",
    "# Second subplot\n",
    "ax = fig.add_subplot(1,2,2, projection='3d')\n",
    "\n",
    "ax.scatter(z, x, y, c=colormap(label), marker='o')\n",
    "\n",
    "ax.set_xlabel('TOTAL DOLLAR VALUE TRADED')\n",
    "ax.set_ylabel('ESTIMATED INCOME')\n",
    "ax.set_zlabel('DAYS SINCE LAST TRADE')\n",
    "\n",
    "plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"split_data\"></a>\n",
    "## 4. Split data into test and train\n",
    "[Top](#top)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,label , random_state=0)\n",
    "\n",
    "print(\"Dimensions of datasets that will be used for training : Input features\"+str(X_train.shape)+ \n",
    "      \" Output label\" + str(y_train.shape))\n",
    "print(\"Dimensions of datasets that will be used for testing : Input features\"+str(X_test.shape)+ \n",
    "      \" Output label\" + str(y_test.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"helper_methods\"></a>\n",
    "## 5. Helper methods for graph generation\n",
    "[Top](#top)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colormap(risk_list):\n",
    "    cols=[]\n",
    "    for l in risk_list:\n",
    "        if l==0:\n",
    "            cols.append('#BB6B5A')\n",
    "        elif l==2:\n",
    "            cols.append('#E5E88B')\n",
    "        elif l==1:\n",
    "            cols.append('#8CCB9B')\n",
    "    return cols\n",
    "\n",
    "def two_d_compare(y_test,y_pred,model_name):\n",
    "    #y_pred = label_encoder.fit_transform(y_pred)\n",
    "    #y_test = label_encoder.fit_transform(y_test)\n",
    "    area = (12 * np.random.rand(40))**2 \n",
    "    plt.subplots(ncols=2, figsize=(10,4))\n",
    "    plt.suptitle('Actual vs Predicted data : ' +model_name + '. Accuracy : %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(X_test['ESTINCOME'], X_test['DAYSSINCELASTTRADE'], alpha=0.8, c=colormap(y_test))\n",
    "    plt.title('Actual')\n",
    "    plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.scatter(X_test['ESTINCOME'], X_test['DAYSSINCELASTTRADE'],alpha=0.8, c=colormap(y_pred))\n",
    "    plt.title('Predicted')\n",
    "    plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "x = X_test['TOTALDOLLARVALUETRADED']\n",
    "y = X_test['ESTINCOME']\n",
    "z = X_test['DAYSSINCELASTTRADE']\n",
    "\n",
    "pop_a = mpatches.Patch(color='#BB6B5A', label='High')\n",
    "pop_b = mpatches.Patch(color='#E5E88B', label='Medium')\n",
    "pop_c = mpatches.Patch(color='#8CCB9B', label='Low')\n",
    "\n",
    "def three_d_compare(y_test,y_pred,model_name):\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    fig.suptitle('Actual vs Predicted (3D) data : ' +model_name + '. Accuracy : %.2f' % accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    ax = fig.add_subplot(121, projection='3d')\n",
    "    ax.scatter(x, y, z, c=colormap(y_test), marker='o')\n",
    "    ax.set_xlabel('TOTAL DOLLAR VALUE TRADED')\n",
    "    ax.set_ylabel('ESTIMATED INCOME')\n",
    "    ax.set_zlabel('DAYS SINCE LAST TRADE')\n",
    "    plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "    plt.title('Actual')\n",
    "\n",
    "    ax = fig.add_subplot(122, projection='3d')\n",
    "    ax.scatter(x, y, z, c=colormap(y_pred), marker='o')\n",
    "    ax.set_xlabel('TOTAL DOLLAR VALUE TRADED')\n",
    "    ax.set_ylabel('ESTIMATED INCOME')\n",
    "    ax.set_zlabel('DAYS SINCE LAST TRADE')\n",
    "    plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "    plt.title('Predicted')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def model_metrics(y_test,y_pred):\n",
    "    print(\"Decoded values of Churnrisk after applying inverse of label encoder : \" + str(np.unique(y_pred)))\n",
    "\n",
    "    skplt.metrics.plot_confusion_matrix(y_test,y_pred,text_fontsize=\"small\",cmap='Greens',figsize=(6,4))\n",
    "    plt.show()\n",
    "    \n",
    "    #print(\"The classification report for the model : \\n\\n\"+ classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_nb\"></a>\n",
    "## 6. Build Naive Bayes classification model\n",
    "[Top](#top)\n",
    "\n",
    "Applies the bayesian theorem to calculate the probabilty of a data point belonging to a particular class. Given the probability of certain related values,the formula to calculate the probabilty of an event B given event A to occur is calculated as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model_name = 'Naive Bayes Classifier'\n",
    "\n",
    "nbClassifier = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "\n",
    "nb_model = Pipeline(steps=[('preprocessor', preprocessorForCategoricalColumns),('classifier', nbClassifier)]) \n",
    "\n",
    "nb_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_nb= nb_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d_compare(y_test,y_pred_nb,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = label_encoder.inverse_transform(y_test)\n",
    "y_pred_nb = label_encoder.inverse_transform(y_pred_nb)\n",
    "model_metrics(y_test,y_pred_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_lrc\"></a>\n",
    "## 7. Build Logistic Regression classification model\n",
    "[Top](#top)\n",
    "\n",
    "A logistic function is applied to the outcome of linear regression. The logistic function is also referred to as sigmoid function. This outputs a value between 0 and 1. We then select a line that depends on the use case, and any data point with probability value above the line is classified into the class represented by 1 and the data point below the line is classified into the class represented by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_name = \"Logistic Regression Classifier\"\n",
    "\n",
    "logisticRegressionClassifier = LogisticRegression(random_state=0,multi_class='auto',solver='lbfgs',max_iter=1000)\n",
    "\n",
    "lrc_model = Pipeline(steps=[('preprocessor', preprocessorForCategoricalColumns),\n",
    "                            ('classifier', logisticRegressionClassifier)]) \n",
    "\n",
    "lrc_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_lrc = lrc_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test = label_encoder.transform(y_test)\n",
    "two_d_compare(y_test,y_pred_lrc,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = label_encoder.inverse_transform(y_test)\n",
    "y_pred_lrc = label_encoder.inverse_transform(y_pred_lrc)\n",
    "model_metrics(y_test,y_pred_lrc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_knn\"></a>\n",
    "## 8. Build K-Nearest classification model\n",
    "[Top](#top)\n",
    "\n",
    "K number of nearest points around the data point to be predeicted are taken into consideration. These K points at this time, already belong to a class. The data point under consideration, is said to belong to the class with which most number of points from these k points belong to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model_name = \"K-Nearest Neighbor Classifier\"\n",
    "\n",
    "knnClassifier = KNeighborsClassifier(n_neighbors = 5, metric='minkowski', p=2)\n",
    "\n",
    "knn_model = Pipeline(steps=[('preprocessorAll',preprocessorForAllColumns),('classifier', knnClassifier)]) \n",
    "\n",
    "knn_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test = label_encoder.transform(y_test)\n",
    "two_d_compare(y_test,y_pred_knn,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = label_encoder.inverse_transform(y_test)\n",
    "y_pred_knn = label_encoder.inverse_transform(y_pred_knn)\n",
    "model_metrics(y_test,y_pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_svm\"></a>\n",
    "## 9. Build Kernel SVM classification model\n",
    "[Top](#top)\n",
    "\n",
    "Support Vector Machines outputs an optimal line of separation between the classes based on the training data served as input. This line of separation is called a hyperplane in a multi dimensional environment. SVM takes outliers that lie pretty close to another class into consideration to derive this separating hyperplane. Once the model is constructed with this hyperplane, any new point to be predicted will now check to see which side of the hyperplane this values lies in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_name = 'Kernel SVM Classifier'\n",
    "\n",
    "svmClassifier = SVC(kernel='rbf', gamma= 'auto')\n",
    "\n",
    "svm_model = Pipeline(steps=[('preprocessorAll',preprocessorForAllColumns),('classifier', svmClassifier)]) \n",
    "\n",
    "svm_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = label_encoder.transform(y_test)\n",
    "#y_pred_svm = label_encoder.transform(y_pred_svm)\n",
    "two_d_compare(y_test,y_pred_svm,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = label_encoder.inverse_transform(y_test)\n",
    "y_pred_svm = label_encoder.inverse_transform(y_pred_svm)\n",
    "model_metrics(y_test,y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_rfc\"></a>\n",
    "## 10. Build Random Forest classification model\n",
    "[Top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree algorithms are efficient in eliminating columns that don't add value in predicting the output and in some cases, we are even able to see how a prediction was derived by backtracking the tree. However, this algorithm doesn't perform individually when the trees are huge and are hard to interpret. Such models are often referred to as weak models. The model performance is however improvised by taking an average of several such decision trees derived from the subsets of the training data. This approach is called the Random Forest classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_name = \"Random Forest Classifier\"\n",
    "\n",
    "randomForestClassifier = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "\n",
    "rfc_model = Pipeline(steps=[('preprocessorAll',preprocessorForAllColumns),('classifier', randomForestClassifier)]) \n",
    "\n",
    "rfc_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_rfc = rfc_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = label_encoder.transform(y_test)\n",
    "two_d_compare(y_test,y_pred_rfc,model_name)\n",
    "\n",
    "#three_d_compare(y_test,y_pred_rfc,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = label_encoder.inverse_transform(y_test)\n",
    "y_pred_rfc = label_encoder.inverse_transform(y_pred_rfc)\n",
    "model_metrics(y_test,y_pred_rfc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"compare_classification\"></a>\n",
    "## 11. Comparative study of different classification algorithms. \n",
    "[Top](#top)\n",
    "\n",
    "In the bar chart below, we have compared the different classification algorithm against the actual values. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues, occurCount = np.unique(y_test, return_counts=True)\n",
    "frequency_actual = (occurCount[0],occurCount[2],occurCount[1])\n",
    "\n",
    "uniqueValues, occurCount = np.unique(y_pred_knn, return_counts=True)\n",
    "frequency_predicted_knn = (occurCount[0],occurCount[2],occurCount[1])\n",
    "\n",
    "uniqueValues, occurCount = np.unique(y_pred_rfc, return_counts=True)\n",
    "frequency_predicted_rfc = (occurCount[0],occurCount[2],occurCount[1])\n",
    "\n",
    "uniqueValues, occurCount = np.unique(y_pred_lrc, return_counts=True)\n",
    "frequency_predicted_lrc = (occurCount[0],occurCount[2],occurCount[1])\n",
    "\n",
    "uniqueValues, occurCount = np.unique(y_pred_svm, return_counts=True)\n",
    "frequency_predicted_svm = (occurCount[0],occurCount[2],occurCount[1])\n",
    "\n",
    "uniqueValues, occurCount = np.unique(y_pred_nb, return_counts=True)\n",
    "frequency_predicted_nb = (occurCount[0],occurCount[2],occurCount[1])\n",
    "\n",
    "n_groups = 3\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.1\n",
    "opacity = 0.8\n",
    "\n",
    "rects1 = plt.bar(index, frequency_actual, bar_width,\n",
    "alpha=opacity,\n",
    "color='g',\n",
    "label='Actual')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, frequency_predicted_nb, bar_width,\n",
    "alpha=opacity,\n",
    "color='pink',\n",
    "label='Naive Bayesian - Predicted')\n",
    "\n",
    "rects3 = plt.bar(index + bar_width*2, frequency_predicted_lrc, bar_width,\n",
    "alpha=opacity,\n",
    "color='y',\n",
    "label='Logistic Regression - Predicted')\n",
    "\n",
    "rects4 = plt.bar(index + bar_width*3, frequency_predicted_knn, bar_width,\n",
    "alpha=opacity,\n",
    "color='b',\n",
    "label='K-Nearest Neighbor - Predicted')\n",
    "\n",
    "rects5 = plt.bar(index + bar_width*4, frequency_predicted_svm, bar_width,\n",
    "alpha=opacity,\n",
    "color='red',\n",
    "label='Kernel SVM - Predicted')\n",
    "\n",
    "rects6 = plt.bar(index + bar_width*5, frequency_predicted_rfc, bar_width,\n",
    "alpha=opacity,\n",
    "color='purple',\n",
    "label='Random Forest - Predicted')\n",
    "\n",
    "plt.xlabel('Churn Risk')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Actual vs Predicted frequency.')\n",
    "plt.xticks(index + bar_width, ('High', 'Medium', 'Low'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we see that the predictions around the Medium values is low on accuracy. One reason for that could be that the number of entries for the Medium values were much less represented than High and Low values. Further testing can be done by either increasing the number of Medium entries or involving several data fabrication techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font size=-1 color=gray>\n",
    "&copy; Copyright 2019 IBM Corp. All Rights Reserved.\n",
    "<p>\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file\n",
    "except in compliance with the License. You may obtain a copy of the License at\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the\n",
    "License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "express or implied. See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
