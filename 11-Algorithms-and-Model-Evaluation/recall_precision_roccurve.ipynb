{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7044d4-2801-4134-aecb-712f59ef0b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56e438e4-f189-4adc-90c5-ffff1772fb54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Recall, Precision, PR Curve and ROC Curve Explained\n",
    "\n",
    "Sources:\n",
    "\n",
    "- Doug Steen, Precision-Recall Curves, https://medium.com/@douglaspsteen/precision-recall-curves-d32e5b290248\n",
    "\n",
    "- Juan C Olamendy, Choosing the Right Metrics: Recall, Precision, PR Curve and ROC Curve Explained, https://medium.com/@douglaspsteen/precision-recall-curves-d32e5b290248\n",
    "\n",
    "- Chris Kuo/Dr. Dataman, Revisiting the ROC and the Precision-Recall Curves, https://medium.com/dataman-in-ai/revisiting-the-roc-and-the-precision-recall-curves-f9c4975b1dd\n",
    "\n",
    "- Maria Gusarova, Understanding AUC — ROC and Precision-Recall Curves, https://medium.com/@data.science.enthusiast/auc-roc-curve-ae9180eaf4f7\n",
    "\n",
    "- Juan Esteban de la Calle, How and Why I Switched from the ROC Curve to the Precision-Recall Curve to Analyze My Imbalanced Models: A Deep Dive, https://juandelacalle.medium.com/how-and-why-i-switched-from-the-roc-curve-to-the-precision-recall-curve-to-analyze-my-imbalanced-6171da91c6b8\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Precision and Recall",
   "id": "dd4b30362f3a60e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Recall**:\n",
    "\n",
    "Recall = True Positives / (True Positives + False Negatives)\n",
    "\n",
    "**Precision:**\n",
    "\n",
    "Precision = True Positives / (True Positives + False Positives)\n",
    "\n",
    "**Example:**\n",
    "- If there were 100 people with a disease and the test correctly identified 80 of them, the recall would be 0.8.\n",
    "- If the test predicted that 50 people had the disease, but only 30 of them actually did, the precision would be 0.6.\n"
   ],
   "id": "85673cca91e90073"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"recallprecision1.png\")"
   ],
   "id": "cffcfe764d0ddd2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "f3c5c92c99d59dc7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9bd94-73a3-43e8-954a-50b77d40473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have the true labels (y_true) and predicted labels (y_pred)\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "y_pred = [1, 1, 1, 1, 0, 0, 0, 1, 1, 0]\n",
    "# Calculate precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "# Calculate recall\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### The Precision-Recall (PR) Curve\n",
    "\n",
    "\n",
    "- In a **PR curve**, precision is plotted on the y-axis, and recall is plotted on the x-axis. \n",
    "- Each point on the curve represents a different threshold value. \n",
    "- As the threshold varies, the balance between precision and recall changes.\n",
    "- **High Precision and Low Recall:** This indicates that the model is very accurate in its positive predictions but fails to capture a significant number of actual positive cases.\n",
    "- **Low Precision and High Recall:** This suggests that the model captures most of the positive cases but at the expense of making more false positive errors."
   ],
   "id": "15db405fd7f3c8fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"recallprecision2.png\")"
   ],
   "id": "4a5897297d9c94ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "f7c3510d182c8f3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd63de-5283-4d02-b78f-1c7e0b5ada81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression classifier\n",
    "# model = LogisticRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "# # Predict probabilities for the test set\n",
    "# y_scores = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "# Assume you have the true labels (y_true) and predicted probabilities (y_scores)\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "y_scores = [0.8, 0.6, 0.9, 0.7, 0.4, 0.6, 0.3, 0.5, 0.8, 0.2]\n",
    "# Compute precision-recall curve\n",
    "# precision is an array of precision values at different thresholds.\n",
    "# recall is an array of recall values at different thresholds.\n",
    "# thresholds is an array of threshold values used to compute precision and recall.\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871f7b3-6270-4025-8e5a-7b8af78b171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf976a-59fa-4c42-acd3-ed84f2ee886d",
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- The **PR curve** can be used to select an appropriate threshold for making predictions. \n",
    "- By examining the curve, you can find the point where precision begins to drop significantly and set the threshold just before this drop.\n",
    "- This allows you to balance both precision and recall effectively. \n",
    "- Once the threshold is identified, predictions can be made by checking whether the model’s score for each instance is greater than or equal to this threshold."
   ],
   "id": "fca8641de906447e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**PR-AUC (Area Under the PR Curve):**\n",
    "- Summary metric that captures the model’s performance across all thresholds.\n",
    "- **Perfect classifier: PR-AUC = 1.0** (perfect precision and recall at all thresholds).\n",
    "- **Random classifier: PR-AUC equal to the proportion of positive labels in the dataset** (no better than chance performance)."
   ],
   "id": "9921625ac37e2a9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### The Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "- Evaluate binary classification models.\n",
    "\n",
    "- Plot the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold settings.\n",
    "\n",
    "- **True Positive Rate (TPR):**\n",
    "    * Also called **recall** or **sensitivity.**\n",
    "    * Ratio of positive instances correctly classified as positive\n",
    "    \n",
    "- **True Negative Rate (TNR):**\n",
    "    * Also called **specificity.**\n",
    "    * Ratio of negative instances correctly classified as negative\n",
    "\n",
    "- **False Positive Rate (FPR):**\n",
    "    * Ratio of negative instances that are incorrectly classified as positive. \n",
    "    * Equal to 1 — True Negative Rate (TNR).\n",
    "      \n",
    "- **ROC-AUC (Area Under the ROC Curve):**\n",
    "    * A single scalar value that summarizes the overall ability of the model to discriminate between the positive and negative classes over all possible thresholds."
   ],
   "id": "2a8d73a126829ba1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad171210-dd9f-4dde-a572-3f9a92d0f537",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import roc_curve, roc_auc_score"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdeb9ab-9d12-491c-9492-a45040da71ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict probabilities for the test set\n",
    "# y_scores = model.predict_proba(X_test)[:, 1]  # probabilities for the positive class\n",
    "# Assume you have the true labels (y_true) and predicted probabilities (y_scores)\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "y_scores = [0.8, 0.6, 0.9, 0.7, 0.4, 0.6, 0.3, 0.5, 0.8, 0.2]\n",
    "# Compute ROC curve and AUC score\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "roc_auc = roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Guess')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "a49d735244f27cef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "180a7f35f8baae5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Curve Analysis of ROC Curve\n",
    "\n",
    "- **A curve closer to the top-left corner** indicates a high sensitivity and specificity: the model is effective in classifying both classes correctly.\n",
    "- **A higher curve** indicates better performance, with the ideal point being in the top left corner of the plot (high TPR, low FPR).\n",
    "- **A curve near the diagonal line** (from bottom-left to top-right) indicates that the classifier is performing no better than random guessing.\n",
    "- **ROC-AUC (Area Under the ROC Curve)** ranges from 0.0 to 1.0:\n",
    "    * 0.5: This indicates a model with no discriminative ability, equivalent to random guessing.\n",
    "    * 1.0: This represents a perfect model that correctly classifies all positive and negative instances.\n",
    "    * < 0.5: This suggests a model that performs worse than random chance, often indicating serious issues in model training or data handling.\n",
    "    * The **ROC-AUC** is particularly useful in scenarios where the class distribution is imbalanced. \n",
    "    * **ROC-AUC** is not affected by the proportion of positive and negative instances."
   ],
   "id": "56bb80baca13cc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Key Benefits of Using **ROC-AUC**\n",
    "\n",
    "- **Robust to Class Imbalance:** Unlike accuracy, ROC-AUC is not influenced by the number of cases in each class, making it suitable for imbalanced datasets.\n",
    "- **Threshold Independence:** It evaluates the model’s performance across all possible thresholds, providing a comprehensive measure of its effectiveness.\n",
    "- **Scale Invariance:** The ROC-AUC is not affected by the scale of the scores or probabilities generated by the model, assessing performance based on the ranking of predictions."
   ],
   "id": "3ef2287c7a8e62ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Threshold Selection using the ROC Curve\n",
    "\n",
    "The ROC curve can be used to select an appropriate threshold for making predictions:\n",
    "\n",
    "- Lowering the threshold means the model starts classifying more instances as positive, increasing recall but potentially decreasing precision. \n",
    "- The trade-off between precision and recall needs to be managed carefully based on the application’s tolerance for false positives.\n",
    "- The point where the precision and recall curves cross might be considered an optimal balance, especially when false positives and false negatives carry similar costs."
   ],
   "id": "bde1f9de82e46569"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### PR Curve vs. ROC Curve\n",
    "\n",
    "**When to Use the PR Curve:**\n",
    "\n",
    "- **Imbalanced Datasets:** \n",
    "    * When the positive class is rare, and the dataset is heavily imbalanced, the PR curve is more informative than the ROC curve. \n",
    "    * In imbalanced datasets with rare positive instances, the ROC curve can be misleading, showing high performance even if the model performs poorly on the minority class.\n",
    "    * Examples: fraud detection, disease diagnosis. \n",
    "- **Costly False Positives:** If false positives are more costly or significant than false negatives, the PR curve is more suitable as it focuses on precision. Example: spam email detection.\n",
    "\n",
    "**When to Use the ROC Curve:**\n",
    "\n",
    "- **More Balanced Datasets:** When the dataset is more balanced or when equal emphasis is placed on the performance regarding both false positives and false negatives, the ROC curve is preferred."
   ],
   "id": "f3ee1ad5e9826bf6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b44f566feb05dae6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Example: Heart Disease Diagnosis",
   "id": "f7300fef14853685"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install ucimlrepo",
   "id": "99f26e4cae1fccac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = heart_disease.data.features \n",
    "y = heart_disease.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(heart_disease.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(heart_disease.variables) "
   ],
   "id": "4cb6de228c659954"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=56)"
   ],
   "id": "dec51098d8d11ecc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "62a32f7fc3461927"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fit a vanilla Logistic Regression classifier and make predictions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_test = clf.predict(X_test)"
   ],
   "id": "9fd3220be85511b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "65d94cc717919aaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to calculate Precision and Recall\n",
    "\n",
    "def calc_precision_recall(y_true, y_pred):\n",
    "    \n",
    "    # Convert predictions to series with index matching y_true:\n",
    "    y_pred = pd.Series(y_pred, index=y_true.index)\n",
    "    \n",
    "    # Instantiate counters:\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    # Determine whether each prediction is TP, FP, TN, or FN:\n",
    "    for i in y_true.index: \n",
    "        if y_true[i]==y_pred[i]==1:\n",
    "           TP += 1\n",
    "        if y_pred[i]==1 and y_true[i]!=y_pred[i]:\n",
    "           FP += 1\n",
    "        if y_pred[i]==0 and y_test[i]!=y_pred[i]:\n",
    "           FN += 1\n",
    "    \n",
    "    # Calculate true positive rate and false positive rate\n",
    "    # Use try-except statements to avoid problem of dividing by 0:\n",
    "    try:\n",
    "        precision = TP / (TP + FP)\n",
    "    except:\n",
    "        precision = 1\n",
    "    \n",
    "    try:\n",
    "        recall = TP / (TP + FN)\n",
    "    except:\n",
    "        recall = 1\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "# Test the function:\n",
    "calc_precision_recall(y_test, y_pred_test)"
   ],
   "id": "9ade563da42888d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "16ee609af898f243"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# LOGISTIC REGRESSION (NO REGULARIZATION)\n",
    "\n",
    "# Fit and predict test class probabilities:\n",
    "lr = LogisticRegression(max_iter=10000, penalty='none')\n",
    "lr.fit(X_train, y_train)\n",
    "y_test_probs = lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Containers for true positive / false positive rates:\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "# Define probability thresholds to use, between 0 and 1:\n",
    "probability_thresholds = np.linspace(0, 1, num=100)\n",
    "\n",
    "# Find true positive / false positive rate for each threshold:\n",
    "for p in probability_thresholds:\n",
    "    \n",
    "    y_test_preds = []\n",
    "    \n",
    "    for prob in y_test_probs:\n",
    "        if prob > p:\n",
    "            y_test_preds.append(1)\n",
    "        else:\n",
    "            y_test_preds.append(0)\n",
    "            \n",
    "    precision, recall = calc_precision_recall(y_test, y_test_preds)\n",
    "        \n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)"
   ],
   "id": "226835f325a149bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b015fb7e3d06f345"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# LOGISTIC REGRESSION (L2 REGULARIZATION)\n",
    "\n",
    "# Fit and predict test class probabilities\n",
    "lr_l2 = LogisticRegression(max_iter=1000, penalty='l2')\n",
    "lr_l2.fit(X_train, y_train)\n",
    "y_test_probs = lr_l2.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Containers for true positive / false positive rates\n",
    "l2_precision_scores = []\n",
    "l2_recall_scores = []\n",
    "\n",
    "# Define probability thresholds to use, between 0 and 1\n",
    "probability_thresholds = np.linspace(0,1,num=100)\n",
    "\n",
    "# Find true positive / false positive rate for each threshold\n",
    "for p in probability_thresholds:\n",
    "    \n",
    "    y_test_preds = []\n",
    "    \n",
    "    for prob in y_test_probs:\n",
    "        if prob > p:\n",
    "            y_test_preds.append(1)\n",
    "        else:\n",
    "            y_test_preds.append(0)\n",
    "            \n",
    "    precision, recall = calc_precision_recall(y_test, y_test_preds)\n",
    "        \n",
    "    l2_precision_scores.append(precision)\n",
    "    l2_recall_scores.append(recall)"
   ],
   "id": "a716fcb3ab1ca5af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8c0b917960bba207"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot precision-recall curve\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(recall_scores, precision_scores, label='Logistic Regression')\n",
    "ax.plot(l2_recall_scores, l2_precision_scores, label='L2 Logistic Regression')\n",
    "baseline = len(y_test[y_test==1]) / len(y_test)\n",
    "ax.plot([0, 1], [baseline, baseline], linestyle='--', label='Baseline')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.legend(loc='center left')"
   ],
   "id": "e497744decb88ef9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1740b3047b29cd09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get AUC-PR scores\n",
    "\n",
    "from sklearn.metrics import auc, average_precision_score\n",
    "\n",
    "print(f'LR (No reg.) AUC-PR: {round(auc(recall_scores, precision_scores),2)}')\n",
    "print(f'LR(L2 reg.) AUC-PR: {round(auc(l2_recall_scores, l2_precision_scores),2)}')\n",
    "print('\\n')\n",
    "print(f'LR (No reg.) Avg. Prec.: {round(average_precision_score(y_test, lr.predict_proba(X_test)[:,1]),2)}')\n",
    "print(f'LR (L2 reg.) Avg. Prec.: {round(average_precision_score(y_test, lr_l2.predict_proba(X_test)[:,1]),2)}')"
   ],
   "id": "4b24b4776e65f2f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c5f7cd5837b1d296"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use sklearn to plot precision-recall curves and ROC curves\n",
    "\n",
    "# from sklearn.metrics import plot_precision_recall_curve\n",
    "# plot_precision_recall_curve(lr, X_test, y_test, name = 'Logistic Regression')\n",
    "# plot_precision_recall_curve(lr_l2, X_test, y_test, name = 'L2 Logistic Regression')\n",
    "\n",
    "from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay\n",
    "PrecisionRecallDisplay.from_estimator(lr, X_test, y_test, name = 'Logistic Regression')\n",
    "PrecisionRecallDisplay.from_estimator(lr_l2, X_test, y_test, name = 'L2 Logistic Regression')\n",
    "\n",
    "RocCurveDisplay.from_estimator(lr, X_test, y_test, name = 'Logistic Regression')\n",
    "RocCurveDisplay.from_estimator(lr_l2, X_test, y_test, name = 'L2 Logistic Regression')"
   ],
   "id": "ae87319a6e9d8a11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f3d76b48a08e723e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bc49fb9695cbaab8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
