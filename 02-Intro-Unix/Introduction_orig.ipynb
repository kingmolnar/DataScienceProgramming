{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16pt; font-weight:bold; color:red; padding-bottom:20px; float:right\">Please rename this file before editing!</p>\n",
    "\n",
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this session is to introduce some basic steps on how to work in our computing environment using the UNIX command line and the Jupyter notebook. In the second part we will use a couple of UNIX commands to work on our first data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNIX and UNIX-like systems\n",
    "\n",
    "| Year | Milestone |\n",
    "|------|-----------|\n",
    "|1969  | Ken Thompson, Dennis Ritchie and others started working on the \"little-used PDP-7 in a corner\" at Bell Labs and |\n",
    "|1971  | **UNIX** 1st edition |\n",
    "|1989  | NeXT Computer was launched by Steve Jobs with the **NeXTSTEP** operating system\n",
    "|1991  | Initial release of **Linux** a Unix-like computer operating system assembled under the model of free and open-source software development by Linux Torvalds|\n",
    "|2001  | Apple launches **Mac OS X**, it's first UNIX-based operating system|\n",
    "\n",
    "Originally, computer manufacturers would ship their proprietary operating systems. Many companies created their own flavor or UNIX, licensing parts of the software from other vendors, and adding their own pieces. This resulted in a huge number of dialects.\n",
    "\n",
    "The game changed with **LINUX**, a UNIX-like operating system that was written from *scratch*. I.e. the implementation mimics the functionality of other UNIX versions but does not use any source code from those flavors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![unix history](https://upload.wikimedia.org/wikipedia/commons/7/77/Unix_history-simple.svg)\n",
    "Source: https://en.wikipedia.org/wiki/History_of_Unix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essential UNIX Commands for File Management\n",
    "\n",
    "![unix file system](http://homepages.uc.edu/~thomam/Intro_Unix_Text/Images/unix_file_system.png)\n",
    "Source: http://homepages.uc.edu/~thomam/Intro_Unix_Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands will be helpful in the Unix environment. Most commands have flags that can be used to modify their function. You can get detailed documentation by typing: `$ man <command name>` at the command prompt (note: ```$`'' is used to denote the prompt, do not type it).\n",
    "\n",
    "\n",
    "- **cd**: change directory.\n",
    "    -  example: `$ cd ~/homework_files/` will change the working directory to ~/homework_files/. Note: \"~\" is a shorthand for your home directory.\n",
    "\n",
    "- **ls**: list items in directory.\n",
    "    -  `$ ls -1` will list the files in single-column format\n",
    "    -  `$ ls -lh` will list the files in long format, with file sizes in human-readable format\n",
    "    -  `$ ls -a` will list all files, including hidden files (those starting with ``.'')\n",
    "\n",
    "- **rm**: remove file(s), e.g.\n",
    "    - `$ rm file.txt` removes file.txt;\n",
    "    - `$ rm file*.txt` (with wildcard) removes all files that begin with ``file'' and end with \".txt\". Note: **rmdir** can be used to remove (empty) directories.\n",
    "\n",
    "- **mv**: move or rename file.  \n",
    "    -  example: `$ mv file.txt newname.txt` will rename file.txt to newname.txt.\n",
    "    -  example: `$ mv file.txt ~/newdir/` will move file.txt to directory newdir.\n",
    "\n",
    "- **cp**: copy file.\n",
    "    -  example: `$ cp file.txt copy.txt` will copy file.txt to copy.txt.\n",
    "\n",
    "- **mkdir**: create a new directory.\n",
    "    -  example: `$ mkdir newdir` will create a new directory called newdir.\n",
    "\n",
    "- **more** and **less**: display contents of a (plain text) file.\n",
    "    -  example: `$ more file.txt` will print the contents of file.txt to the standard output.\n",
    "\n",
    "- **head**: show the first few lines of a file (you can change the number of lines using the -n flag).\n",
    "\n",
    "- **tail**: show the last few lines of a file (you can change the number of lines using the -n flag).\n",
    "\n",
    "- **wc**: count words in a document. Without any additional commands, gives line, word, and byte counts for a file.  \n",
    "    -  `$ wc -l file.txt` will count the lines in document file.txt.\n",
    "\n",
    "- **grep**: find specific word in a document.\n",
    "    -  `$ grep -n 'list' file.txt` will display lines with the word 'list' in them (default behavior).\n",
    "    -  `$ grep -l 'list' .` will return the name of the files in the current directory ('.') that match the pattern 'list'.\n",
    "    -  `$ grep -r 'list' .` will search the directory '.' recursively (i.e., including subdirectories).\n",
    "    -  `$ grep -o 'list' .` will return only the matching part of the line.\n",
    "    -  `$ grep -c 'list' .` will count the number of matching lines.\n",
    "    -  You can use extended regular expressions with the `'-E'` flag.\n",
    "\n",
    "- **sort**: return a sorted lines of text.\n",
    "    -  `$ sort -u file.txt` returns only unique values in the sort (no repeats)\n",
    "    -  `$ sort -f` ignores case (ie, converts everything to lowercase)\n",
    "\n",
    "- **tr**: replace (or delete) characters from standard input, write to standard output.\n",
    "    -  `$ tr -d 'a'` will delete all occurences of 'a' in the input, instead of replacing them.\n",
    "    -  `$ tr -s 'a' 'A'` will 'squeeze' any adjacent 'a's into a single occurence, and replace it with 'A', so for example 'aaaarrrgh' will be replaced by 'Arrrgh'.\n",
    "    -  You can do more than one replacement at a time, for example\n",
    "    `$ tr -s 'ar' 'AR'` will convert 'aaaarrrgh' to 'ARgh'.\n",
    "\n",
    "- **wget** and **curl**: non-interactive downloading of files from the Web. You may use it later in the semester to download datasets.\n",
    "\n",
    "-  **(un)zip**, **g(un)zip**, **zcat**, and **tar**: can be used to compress files or to uncompress archives.\n",
    "\n",
    "- **exit**: end current shell session and close.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "First we're going checkout the directory structure. The class directory and GitHub repository have been setup for every student prior to class.\n",
    "\n",
    "1. Login in via SSH into the server `arc.insight.gsu.edu`\n",
    "2. Check that you are in your **home** directory, and see what files already exist\n",
    "\n",
    "\n",
    "**Here are the steps that were performed for you**\n",
    "\n",
    "1. Create a new directory named `IFI8410F23` (this is case sensitive!)\n",
    "4. Navigate into this new directory, i.e. changing it to your current **working directory**\n",
    "5. Use your web-browser to find the class repository on GitHub and follow the instructions on how to clone it (via HTTP). Copy the corresponding URL into your clipboard.\n",
    "6. On the server run the command\n",
    "<pre>$ git clone URL</pre>\n",
    "Where `URL` is the one from the GitHub website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T12:04:13.689732",
     "start_time": "2017-08-20T12:04:13.671139"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd \n",
    "ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping the respository updated\n",
    "\n",
    "- Once you cloned the repository you can download any updates with a single command `git pull`\n",
    "- **Note:** your working directory has to be the repository or one of its subdirectories. Navigate into the right directory before executing the command\n",
    "- **Warning:** Make sure to rename a file before editing. If you edit a file that exists in the repository your pull request will fail. If that happens rename or move the offending file and pull again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Concept of Pipes and Redirections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![unix-pipe](https://upload.wikimedia.org/wikipedia/commons/f/f6/Pipeline.svg)\n",
    "Source: https://en.wikipedia.org/wiki/Pipeline_(Unix)\n",
    "\n",
    "Examples\n",
    "- `ls -l | more`\n",
    "- `ls -R > all_my_files.txt`\n",
    "- `cat`\n",
    "\n",
    "- The standard input **STDIN** can also be connect to a file using the `<` symbol\n",
    "- The standard output **STDOUT** can be redirected to a file using the `>` or `>>` symbols. Hereby `>>` appends the new content to an existing file.\n",
    "- The standard error **STDERR** can be redirected to file using `2>` and `2>>`. The symbol `2>&1` merges STDOUT and STDERR into one stream.\n",
    "\n",
    "## Examples\n",
    "\n",
    "- `ls -l | more`\n",
    "- `ls -R > all_my_files.txt`\n",
    "- `cat all_my_files.txt | grep foo > data.dat`\n",
    "- `sort < data.dat`\n",
    "- `echo -n \"The number of README files in this directory is: \"; find . -name \"README*\" | wc -l`\n",
    "- ``echo -n \"There are `find . -name \"README*\" | wc -l` README files in this directory tree.\" ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T16:00:25.673262",
     "start_time": "2018-02-08T16:00:25.646469"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 32 README files in the parent directory tree."
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "echo -n \"There are `find .. -name \"README*\" | wc -l` README files in the parent directory tree.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNIX Commands/Tools for Data Manipulation and Analysis\n",
    "\n",
    "- `grep`\n",
    "- `sort`\n",
    "- `uniq`\n",
    "- `tr`\n",
    "- `wc`\n",
    "- `cut`\n",
    "\n",
    "Advanced\n",
    "\n",
    "- `awk`\n",
    "- `sed`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Works  by Shakespeare\n",
    "Let's see what we can do with Shakespeare's collected body of work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download everything from http://www.gutenberg.org/ebooks/100 in plain text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T13:01:47.379370",
     "start_time": "2017-08-20T13:01:46.874366"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir -p data\n",
    "cd data\n",
    "wget https://www.gutenberg.org/cache/epub/100/pg100.txt 2> /dev/null\n",
    "cd ..\n",
    "ls -l data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the file in the **terminal** using the commands `more` or `less`\n",
    "\n",
    "There is a lot of \"junk\":\n",
    "- There is a lot of other text like legal notices included.\n",
    "- Special character and even upper and lower case words will affect the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tricks\n",
    "- Use `tr -s ' '` to squash duplicate characters (banks in this case)\n",
    "- Filter entire repeating passages by\n",
    "    1. copy those lines into a file\n",
    "    2. run `grep -v -f data/legalnotice.txt < data/pg100.txt`\n",
    "- Filter empty/blank lines with `grep -v -e '^[[:space:]]*$'` this those are the little things you find on StackOverflow https://stackoverflow.com/questions/3432555/remove-blank-lines-with-grep ... usually after some digging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some questions?\n",
    "\n",
    "- How often do the terms \"love\", \"hate\", \"murder\", \"faith\" appear in the text?\n",
    "- What are the most frequent words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T13:57:44.976850",
     "start_time": "2017-08-20T13:57:44.927191"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "cat data/pg100.txt | grep hate | head -20\n",
    "echo\n",
    "echo -n \"The answer is: \"\n",
    "cat data/pg100.txt | grep hate | wc -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T14:00:09.910175",
     "start_time": "2017-08-20T14:00:09.855299"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "cat data/pg100.txt | tr ' ' '\\n' | grep hate | sort | uniq -c | sort -rn | head -20\n",
    "echo\n",
    "echo -n \"The answer is: \"\n",
    "cat data/pg100.txt | tr ' ' '\\n' | grep hate | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. But this could be improved..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T14:07:47.690876",
     "start_time": "2017-08-20T14:07:47.684077"
    }
   },
   "source": [
    "What are the most frequent words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T14:08:58.043012",
     "start_time": "2017-08-20T14:08:54.202453"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "grep -v -f data/legalnotice.txt data/pg100.txt \\\n",
    "| tr -d '.,:?\"' \\\n",
    "| tr 'A-Z' 'a-z' \\\n",
    "| tr ' ' '\\n' \\\n",
    "| tr -s '\\n' \\\n",
    "| grep -v -e '^[[:space:]]*$' \\\n",
    "| sort \\\n",
    "| uniq -c \\\n",
    "| sort -rn \\\n",
    "| head -30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Groups Dataset\n",
    "http://qwone.com/~jason/20Newsgroups/\n",
    "\n",
    "he 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "<p style=\"font-size:20pt; color:red;\">DO NOT DOWNLOAD THESE FILES ONTO THE CLUSTER!</p>\n",
    "Instead, use the shared data directory `/home/data/20_newsgroup/`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T14:21:12.678214",
     "start_time": "2017-08-20T14:21:12.559993"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "DATADIR=/data/textanalysis/20_newsgroups\n",
    "ls $DATADIR | while read TOPIC; do\n",
    "echo -n \"Topic $TOPIC: number of documents \"\n",
    "ls $DATADIR/$TOPIC | wc -l\n",
    "done | cat -b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some questions\n",
    "- What are the most frequent words in each topic?\n",
    "- Is there a certain set of words that is unique to a particular topic?\n",
    "- Can we score documents based on how often those topic specific words appear?\n",
    "- Is it possible to determine the topic of an unknown document by this score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts with a Hash-Bang\n",
    "Even in the few examples above we have used the same sequence of commands over and over again. We should create new commands (or scripts) for those steps.\n",
    "\n",
    "How to create a script:\n",
    "1. Create a text file with the name of your new \"command\". We often add something like \".sh\" or \".py\" to indicate which language the script is written in. E.g. `wordfrequency.sh`\n",
    "2. The very first line of the text file must indicate the interpreter that is going to execute the program. In our case `#!/bin/bash`\n",
    "3. The executable permissions need to be set so that we can use the new script just like any other command. This is done with `chmod a+x wordfrequency.sh`\n",
    "\n",
    "Our script would look something like this:\n",
    "```\n",
    "#!/bin/bash\n",
    "tr -d '.,:?\"' \\\n",
    "| tr 'A-Z' 'a-z' \\\n",
    "| tr ' ' '\\n' \\\n",
    "| grep -v -e '^[[:space:]]*$' \\\n",
    "| sort \\\n",
    "| uniq -c \\\n",
    "| sort -rn\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "nav_menu": {
    "height": "264px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
