{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from IPython.display import Image",
   "id": "13c86303088d6bbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression and Overfitting / Underfitting\n",
    "\n",
    "Source: \n",
    "\n",
    "- RITHP, **Logistic Regression and regularization: Avoiding overfitting and improving generalization,** https://medium.com/@rithpansanga/logistic-regression-and-regularization-avoiding-overfitting-and-improving-generalization-e9afdcddd09d\n",
    "- Nilesh Parashar, **From Generalization to Overfitting: The Science Behind Data Overfitting,** https://medium.com/@niitwork0921/from-generalization-to-overfitting-the-science-behind-data-overfitting-65f5c6901729\n",
    "- Charles Chi, **Overfitting and Underfitting during Model Training,** https://medium.com/ai-assimilating-intelligence/overfitting-and-underfitting-in-model-training-e0b14a89bd49\n",
    "- Ivan Zakharchuk, **Generalization, Overfitting, and Under-fitting in Supervised Learning,** https://ivanzakharchuk.medium.com/generalization-overfitting-and-underfitting-in-supervised-learning-a21f02ebf3df\n"
   ],
   "id": "6f986f4e4e351db2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Overfitting in logistic regression:\n",
    "\n",
    "- Occurs when the model has too many parameters (degrees of freedom) relative to the size of the training data.\n",
    "- Leads to high training accuracy but low test accuracy. "
   ],
   "id": "8e979ce46ea0715e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Image(filename=\"figures/regularization_1.png\", width=500)",
   "id": "1452d7120901c3d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Image(filename=\"figures/regularization_2.png\", width=500)",
   "id": "99dde3b5f6cb2cf0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Regularization:\n",
    "\n",
    "- Technique to avoid overfitting by adding a **penalty term** to the objective function (loss function) that the model is trying to minimize. \n",
    "- The penalty term **reduces the complexity of the model**.\n",
    "- It improves its generalization by **reducing the variance** of the model."
   ],
   "id": "e0e20f5d1f95bd32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Types of regularization:\n",
    "\n",
    "- **L1 (Lasso)** regularization:\n",
    "    * Adds a penalty term to the objective function equal to the absolute value of the coefficients.\n",
    "    * Leads to a sparse model, where many of the coefficients are exactly equal to zero.\n",
    "    * Useful for feature selection because it can automatically identify and remove unnecessary or redundant features.\n",
    "      \n",
    "- **L2 (Ridge)** regularization (default choice):\n",
    "    * Adds a penalty term to the objective function equal to the square of the coefficients.\n",
    "    * Leads to a model with all coefficients close to zero, but not necessarily equal to zero.\n",
    "    * Less prone to overfitting than L1 regularization.\n",
    "      \n",
    "- **Elastic Net** regularization:\n",
    "    * Combines L1 and L2 regularization by adding a penalty term that is a combination of the absolute value and square of the coefficients.\n",
    "    * Leads to a model with some coefficients equal to zero and some close to zero.\n",
    "    * Useful when there are correlated features in the data (Lasso is prone to selecting only one of them)."
   ],
   "id": "d1d93f8138b53ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Regularization procedure:\n",
    "\n",
    "- Choose strength of regularization (hyperparameter) via cross-validation.\n",
    "- Select the hyperparameter that gives the best performance on the validation set. \n",
    "- Try a range of values (e.g. 10^-6 to 10⁶) and use a logarithmic scale. \n",
    "- Use a grid search to try a range of values for multiple hyperparameters at once.\n",
    "\n",
    "\n",
    "**Other techniques:**\n",
    "\n",
    "- **Feature selection and dimensionality reduction methods**: \n",
    "    * Extract the most important and useful traits while eliminating the rest. \n",
    "\n",
    "- **Ensemble techniques:**\n",
    "    * Ensemble techniques like **bagging** and **boosting** integrate numerous models. \n",
    "    * Ensemble approaches improve generalization.\n",
    "    * Mitigate the effect of individual model biases by combining the predictions of several models.\n",
    "      \n",
    "- **Boosting the size of the data used for training** and **simplifying the model**\n",
    "\n",
    "- **Pruning (for decision trees):** \n",
    "    * Pruning reduces the size of decision trees by cutting off branches that have little importance, thus simplifying the model and reducing the risk of overfitting.\n",
    "\n",
    "- **Dropout (for neural networks):**\n",
    "    * Dropout randomly disables a fraction of neurons during training\n",
    "    * Helps prevent the network from becoming overly dependent on any specific set of features, i.e. helps generalize."
   ],
   "id": "ff7f097f7ff679e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### How to implement logistic regression with regularization in python:\n",
    "\n",
    "- Use the `LogisticRegression` class in `scikit-learn` with the **“penalty”** and **“C”** hyperparameters\n",
    "\n",
    "- Set the **“penalty”** hyperparameter to **“l1”, “l2”,** or **“elasticnet”.**\n",
    "\n",
    "- Set the **“C”** hyperparameter to the regularization strength.\n",
    "\n",
    "- The “C” hyperparameter controls the strength of the regularization:\n",
    "    * A smaller value for “C” (e.g. C=0.01) leads to stronger regularization and a simpler model.\n",
    "    * A larger value (e.g. C=1.0) leads to weaker regularization and a more complex model."
   ],
   "id": "99fb13e4e0593193"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Example:",
   "id": "63629d5fc2bab872"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "df_churn_pd = pd.read_csv(\"/data/IFI8410/sess09/mergedcustomers_missing_values_GENDER.csv\")\n",
    "df_churn_pd.head()"
   ],
   "id": "998f2999c4fade1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#remove columns that are not required\n",
    "df_churn_pd = df_churn_pd.drop(['ID'], axis=1)"
   ],
   "id": "c3ca4b1f3fa771c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# prepare data frame for splitting data into train and test datasets\n",
    "features = []\n",
    "features = df_churn_pd.drop(['CHURNRISK'], axis=1)\n",
    "\n",
    "label_churn = pd.DataFrame(df_churn_pd, columns = ['CHURNRISK']) \n",
    "label_encoder = LabelEncoder()\n",
    "label = df_churn_pd['CHURNRISK']\n",
    "\n",
    "label = label_encoder.fit_transform(label)\n",
    "print(\"Encoded value of Churnrisk after applying label encoder : \" + str(label))"
   ],
   "id": "814eeca7ffe984d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the data\n",
    "X = features\n",
    "y = label\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "1ce490bbd398c28d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### L1 regularization",
   "id": "1648246ab4540ed4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set the regularization type (L1, L2, Elastic Net)\n",
    "penalty = 'l1'\n",
    "\n",
    "# Set the regularization strength (C)\n",
    "C = 0.01\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression(penalty=penalty, C=C)\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f'Test accuracy: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "# Set the regularization strength (C)\n",
    "C = 0.1\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression(penalty=penalty, C=C)\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f'Test accuracy: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "# Set the regularization strength (C)\n",
    "C = 1.0\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression(penalty=penalty, C=C)\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f'Test accuracy: {accuracy:.2f}')"
   ],
   "id": "d23ef270f98d1cdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### L2 regularization",
   "id": "3770a63eb39898cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set the regularization type (L1, L2, Elastic Net)\n",
    "penalty = 'l2'\n",
    "\n",
    "# Set the regularization strength (C)\n",
    "C = 0.01\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression(penalty=penalty, C=C)\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f'Test accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Set the regularization strength (C)\n",
    "C = 0.1\n",
    "\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression(penalty=penalty, C=C)\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f'Test accuracy: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "# Set the regularization strength (C)\n",
    "C = 1.0\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression(penalty=penalty, C=C)\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f'Test accuracy: {accuracy:.2f}')"
   ],
   "id": "248788c0c4f46dde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Elastic net regularization",
   "id": "8070f21345c30c73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set the regularization type (L1, L2, Elastic Net)\n",
    "penalty = 'elasticnet'\n",
    "\n",
    "# Set the regularization strength (C)\n",
    "C = 0.01\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression(penalty=penalty, C=C)\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f'Test accuracy: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "# Set the regularization strength (C)\n",
    "C = 0.1\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression(penalty=penalty, C=C)\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f'Test accuracy: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "# Set the regularization strength (C)\n",
    "C = 1.0\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression(penalty=penalty, C=C)\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f'Test accuracy: {accuracy:.2f}')"
   ],
   "id": "cf57403bb88b7771"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eecb5a72d7bd79e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Underfitting:\n",
    "\n",
    "- **Increase Model Complexity:** \n",
    "    * A more complex model is necessary to capture the nuances of your data.\n",
    "    * Add more layers to a neural network.\n",
    "    * Use a more sophisticated algorithm to learn more complex patterns.\n",
    "\n",
    "- F**eature Engineering:** \n",
    "    * Improve model performance by creating new features.\n",
    "    * The right features can be transformations which better highlight the relationships between variables. \n",
    "\n",
    "- **Reduce Regularization:**\n",
    "    * Reduce the strength of regularization.\n",
    "\n",
    "- **Increase Training Time:**\n",
    "    * Allow more time for training.\n",
    "    * Run more epochs or iterations.\n",
    "    * Giving the model additional opportunities to learn from the data. Related: tweaking learning rate may help as well.\n",
    "\n",
    "- **Add More Data:**\n",
    "    * If feasible, incorporate more training data to help the model to identify and learn the underlying structure of the dataset. \n",
    "    * More data provides a broader representation of the problem space, potentially improving the model’s accuracy and generalization."
   ],
   "id": "27e5f435c54d3b02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Image(filename=\"figures/regularization_3.png\", width=500)",
   "id": "1a95a94c0b8bd84f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Image(filename=\"figures/regularization_4.png\", width=500)",
   "id": "2503235d5825f3f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "12f1e35430385c7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e150e3caf808b683"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
