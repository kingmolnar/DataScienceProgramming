{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:left;color:red;font-weight:bold;font-size:16pt;padding-bottom:20px\">Please, copy this notebook before editing!</p>\n",
    "\n",
    "# Homework 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tuning\n",
    "\n",
    "In this exercise, you experiment with hyper-parameter tuning. You use XGBoost \n",
    "\n",
    "Examples are takeng from this blog post\n",
    "https://towardsdatascience.com/xgboost-fine-tune-and-optimize-your-model-23d996fab663\n",
    "\n",
    "\n",
    "## Data Set\n",
    "\n",
    "| Data Set Characteristics | |\n",
    "|-|-|\n",
    "| Number of Instances | 20,640 |\n",
    "| Number of Attributes| 8 numeric, predictive attributes and the target |\n",
    "| Missing Attribute Values | None | \n",
    "\n",
    "\n",
    "| Attribute | Information |\n",
    "|-|-|\n",
    "| `MedInc` | median income in block group |\n",
    "| `HouseAge` | median house age in block group |\n",
    "| `AveRooms` | average number of rooms per household |\n",
    "| `AveBedrms` | average number of bedrooms per household |\n",
    "| `Population` | block group population |\n",
    "| `AveOccup` | average number of household members |\n",
    "| `Latitude` | block group latitude |\n",
    "| `Longitude` | block group longitude |\n",
    "\n",
    "\n",
    "This dataset was obtained from the StatLib repository. https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
    "\n",
    "The **target variable** `MedHouseVal` is the median house value for California districts, expressed in hundreds of thousands of dollars ($100,000).\n",
    "\n",
    "This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
    "\n",
    "A household is a group of people residing within a home. Since the average number of rooms and bedrooms in this dataset are provided per household, these columns may take surprisingly large values for block groups with few households and many empty houses, such as vacation resorts.\n",
    "\n",
    "\n",
    "#### References\n",
    "\n",
    "Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33 (1997) 291-297\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Split Data Set\n",
    "Write a function `split_dataframe` that splits a given DataFrame randomly into two sets. The second argument of the function is the `sample_ratio`, that determines the size of the first set. E.g. `sample_ratio` = 0.8 means that 80% of the data go into the first set, and 20% into the second. The functions returns a tuple of two DataFrames.\n",
    "\n",
    "One way to split randomly:\n",
    "1. create a (NumPy) vector of random values between 0 and 1. The vector is of the length of the data set.\n",
    "2. create a boolean vector by comparing the vector elements to the `sample_ratio`. E.g. `mask = (np.random.rand(df.shape[0]) <= sample_ratio)`\n",
    "3. use `mask` to select the first data set, and the negation `~mask` for the second.\n",
    "4. returns copies of the new dataframes (with `.copy()`) so that you can easily modify the resulting dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import fetch_california_housing\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "housing_df = pd.read_csv('/data/IFI8410/california_housing/california_housing.csv')\n",
    "print(f\"Number of records: {housing_df.shape[0]:,}\")\n",
    "housing_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your solution to `solution_10_1.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile solution_10_1.py\n",
    "\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def split_dataframe(df: pd.DataFrame, sample_ratio: float = 0.8) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    ### your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your code\n",
    "from solution_10_1 import split_dataframe\n",
    "import pandas as pd\n",
    "\n",
    "housing_df = pd.read_csv('/data/IFI8410/california_housing/california_housing.csv')\n",
    "print(f\"Number of records: {housing_df.shape[0]:,}\")\n",
    "\n",
    "housing_train, housing_test = split_dataframe(housing_df, 0.7)\n",
    "del housing_df \n",
    "print(f\"Number of records in training set: {housing_train.shape[0]:,}\")\n",
    "print(f\"Number of records in test set: {housing_test.shape[0]:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 10.1 Execute the cell below to test your solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! test/run_test.sh 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Problem Statement\n",
    "It is important that the training and test sets have the same value distribution. The function `compare_feature_distributions` computes the relative differences in the statistical disctributions of the two data sets for each feature.\n",
    "\n",
    "The resulting table should show values very close to 0. Specifically for the mean, standard deviation, and the quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compare_feature_distributions(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Create a joined table of the \"descriptions\" of each table\n",
    "    # Note: we use the transposed DataFrame, so that the features become row indices, and the metrics become columns\n",
    "    comp = df1.describe().T.join(df2.describe().T, lsuffix='_one', rsuffix='_two')\n",
    "    mets = ['mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "    # Calculate the value range for each feature\n",
    "    comp['delta'] = comp.apply(lambda r: max(r.max_one, r.max_two) - min(r.min_one, r.min_two), axis=1)\n",
    "    for met in mets:\n",
    "        # Calculate the relative difference for each metric\n",
    "        comp[f\"diff_{met}\"] = (comp[f\"{met}_one\"] - comp[f\"{met}_two\"])/comp['delta']\n",
    "    res_cols = [ f\"diff_{met}\" for met in mets ]\n",
    "    # return a new copy of the subset of the DataFrame\n",
    "    return comp[res_cols].copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try is out on your train and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_df = compare_feature_distributions(housing_train, housing_test)\n",
    "# save the comparison\n",
    "cmp_df.to_csv('output_10_2.csv', index_label='feature')\n",
    "display(cmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 10.2 Execute the cell below to test your solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! test/run_test.sh 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Data Pre-Processing\n",
    "Implement a pre-processor (aka `ColumnTransformer`) to apply the standard scaler to the feature columns, **but not** the target feature `MedHouseVal`.\n",
    "The pre-processor that the function returns must be already trained, aka `.fit()` (on the training set)\n",
    "\n",
    "Use the example in Homework 7 to build a pre-processor:\n",
    "- Create an instance of `StandardScaler`\n",
    "- Create a pipeline object (`Pipeline`) with the standard scaler as step\n",
    "- Create a list of numerical feature columns without the target feature `MedHouseVal`. (You can extract them from the dataframe)\n",
    "- Create a `ColumnTransformer` object with the pipeline for the numeric columns, use `remainder=\"passthrough\"` to bypass all other columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your solution to `solution_10_3.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile solution_10_3.py\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def build_preprocessor(training_df, target_feature: str = 'MedHouseVal') -> ColumnTransformer:\n",
    "    ### your code here\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try your code\n",
    "\n",
    "from solution_10_1 import split_dataframe\n",
    "from solution_10_3 import build_preprocessor\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/data/IFI8410/california_housing/california_housing.csv')\n",
    "df_train, df_test = split_dataframe(df, 0.6)\n",
    "\n",
    "# Create pre-processor\n",
    "pre_processor = build_preprocessor(df_train, target_feature='MedHouseVal')\n",
    "\n",
    "# Check distribution\n",
    "print(\"Training Set\")\n",
    "Xy_train = pre_processor.transform(df_train)\n",
    "display(Xy_train.mean(axis=0))\n",
    "display(Xy_train.std(axis=0))\n",
    "\n",
    "print(\"Test Set\")\n",
    "Xy_test = pre_processor.transform(df_test)\n",
    "display(Xy_test.mean(axis=0))\n",
    "display(Xy_test.std(axis=0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 10.3 Execute the cell below to test your solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! test/run_test.sh 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 Train Regression Model\n",
    "\n",
    "Use [XGBoost](https://xgboost.readthedocs.io/en/stable/) for to build a regression model.\n",
    "\n",
    "XGBoost is a popular algorithm that can be used for regression and classification.\n",
    "\n",
    "This article gives a nice introduction to XGBoost: <https://towardsdatascience.com/xgboost-fine-tune-and-optimize-your-model-23d996fab663>\n",
    "\n",
    "Execute the cells below to see how it's used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solution_10_1 import split_dataframe\n",
    "from solution_10_3 import build_preprocessor\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "\n",
    "df = pd.read_csv('/data/IFI8410/california_housing/california_housing.csv')\n",
    "# Let's split the data 80:20\n",
    "df_train, df_test = split_dataframe(df, 0.8)\n",
    "\n",
    "# Create pre-processor\n",
    "pre_processor = build_preprocessor(df_train, target_feature='MedHouseVal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps process the training data and package them for model training.\n",
    "XGBoost is not part of the Scikit-Learn package. So, some if the data structures are different. Specifically, the Python XGBoost package has its own structure `DMatrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train = pre_processor.transform(df_train)\n",
    "X = Xy_train[:, :-1]\n",
    "y = Xy_train[:, -1]\n",
    "print(X.shape, y.shape)\n",
    "dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "print(f\"Number of rows: {dmatrix.num_row()}, number of columns: {dmatrix.num_col()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of hyper-parametes. Check the [documentation](https://xgboost.readthedocs.io/en/stable/) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={ 'objective':'reg:squarederror',\n",
    "         'max_depth': 6, \n",
    "         'colsample_bylevel':0.5,\n",
    "         'learning_rate':0.01,\n",
    "         'random_state':20}\n",
    "\n",
    "# Train model\n",
    "model_xgb = xgb.train(dtrain=dmatrix, params=params, num_boost_round=1000)\n",
    "\n",
    "# Save model\n",
    "model_xgb.save_model('model_10_4.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_test = pre_processor.transform(df_test)\n",
    "X_test = Xy_test[:, :-1]\n",
    "y_test = Xy_test[:, -1]\n",
    "dmatrix_test = xgb.DMatrix(data=X_test)\n",
    "print(f\"Number of rows: {dmatrix_test.num_row()}, number of columns: {dmatrix_test.num_col()}\")\n",
    "\n",
    "y_hat = model_xgb.predict(dmatrix_test)\n",
    "RSME = np.sqrt(np.square(y_hat - y_test).mean())\n",
    "print(f\"RMSE: {RSME:.2f}\")\n",
    "\n",
    "# Add predicted values to test dataframe and save\n",
    "df_test['Predicted_MedHouseVal'] = y_hat\n",
    "df_test.to_csv('output_10_4.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 10.4 Execute the cell below to test your solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! test/run_test.sh 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5 Hyper-parameter Optimization\n",
    "\n",
    "You need to experiment with different parameters to improve the model. The following shows an example of the parameter space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = { 'max_depth': [3, 5, 6, 10, 15, 20],\n",
    "           'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "           'subsample': np.arange(0.5, 1.0, 0.1),\n",
    "           'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n",
    "           'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n",
    "           'n_estimators': [100, 500, 1000]}\n",
    "n_params = np.array([ len(v) for k,v in param_space.items() ]).prod()\n",
    "print(f\"Number of parameter combinations: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a randomize search algorithm to experiment with different paramer sets.\n",
    "\n",
    "**This will take a while!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr = xgb.XGBRegressor(seed = 20)\n",
    "clf = RandomizedSearchCV(estimator=xgbr,\n",
    "                         param_distributions=param_space,\n",
    "                         scoring='neg_mean_squared_error',\n",
    "                         n_iter=25,\n",
    "                         verbose=1)\n",
    "T_0 = datetime.datetime.now()\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(f\"Elapsed time: {datetime.datetime.now()-T_0}\")\n",
    "print(\"Best parameters:\", clf.best_params_)\n",
    "\n",
    "# Save parametes\n",
    "json.dump(clf.best_params_, open('output_10_5.json', 'w', encoding='utf-8'), indent=4)\n",
    "\n",
    "# Evaluate\n",
    "y_hat2 = clf.predict(X_test)\n",
    "RMSE_2 = np.sqrt(np.square(y_hat2 - y_test).mean())\n",
    "print(f\"Best RMSE: {RMSE_2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 10.5 Execute the cell below to test your solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! test/run_test.sh 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Execute the cell below to run all tests..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! test/run_test.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Submission\n",
    "- This homework is due by 2024-04-26, 5:30PM (EDT)\n",
    "- Make sure that all your programs and output files are in the exact folder as specified in the instructions.\n",
    "- All file names on this system are case sensitive. Verify if you copy your work from a local computer to your home directory on ARC.\n",
    "\n",
    "**Run the cell below to submit your work.**\n",
    "\n",
    "- You may submit your work multipe times up to the deadline of the assignment.\n",
    "- Please note that any files that you previously submitted will\n",
    "be overwritten by your current files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./submit.sh -y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
