{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvDJJwxUzUCU"
   },
   "source": [
    "\n",
    "\n",
    "# **IFI8410: Programming for Business**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKAsLR0C2DE8"
   },
   "source": [
    "## **Assignment 09**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change the content of this cell. Execute this cell first, and everytime after you restarted the kernel.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Multiple linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiXz48oz2NhW"
   },
   "source": [
    "### Problem 9.1: Data Exploration and Preprocessing for House Price Prediction\n",
    "\n",
    "**Question:**\n",
    "\n",
    "Analyze and preprocess the **HousePricePrediction.csv** dataset to prepare it for a machine learning model that predicts house prices.\n",
    "\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "- Load the dataset and perform an initial exploration to understand the distributions and relationships of each feature with price.\n",
    "\n",
    "- Identify and Handle Missing Values (if any), filling them with appropriate values.\n",
    "\n",
    "- Transform Features:\n",
    "\n",
    "    * Convert date information to a more useful format, such as extracting the year and month as separate features.\n",
    "\n",
    "    * One-hot encode boolean features **has_basement, renovated, nice_view, perfect_condition, has_lavatory,** and **single_floor**.\n",
    "\n",
    "- Normalize Numeric Columns: Normalize or scale numerical features **bedrooms, grade, real_bathrooms, living_in_m2,** and **quartile_zone**.\n",
    "\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Use `pd.to_datetime()` to convert the date column and extract year and month.\n",
    "  \n",
    "- Fill missing values with either the median (for numerical columns) or the mode (for categorical columns).\n",
    "\n",
    "- Normalize or scale all numerical features to a range suitable for machine learning models.\n",
    "\n",
    "\n",
    "**Expected Output**:\n",
    "\n",
    "- Return a cleaned **pandas DataFrame df** that can be used in further modeling tasks.\n",
    "\n",
    "- This DataFrame should be free of missing values, contain transformed features, and have normalized numerical columns.\n",
    "\n",
    "\n",
    "**Hint**:\n",
    "\n",
    "- Use pandas function `pd.to_datetime(. , errors='coerce')` to convert **date** into `datetime` as datatype.\n",
    "You can use this code block:\n",
    "    \n",
    "    `df['date'] = pd.to_datetime(df['date'], errors='coerce')`\n",
    "\n",
    "    `df['year'] = df['date'].dt.year`\n",
    "\n",
    "    `df['month'] = df['date'].dt.month`\n",
    "\n",
    "    `df = df.drop(columns=['date'])`\n",
    "  \n",
    "- Separate the numerical and boolean columns via:\n",
    "\n",
    "    `numeric_cols = df.select_dtypes(\n",
    "        include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    ).columns`\n",
    "\n",
    "    `bool_cols = df.select_dtypes(include=['bool']).columns`\n",
    "\n",
    "- Impute numeric features with their **median** values and boolean features with their **most frequently occurring value**. Use scikit-learn's `SimpleImputer` class for that.\n",
    "\n",
    "- Use `pd.get_dummies()` to one-hot encode boolean features, ensuring all values are in numeric form. Note: You will have to convert the boolean data type into integer. \n",
    "\n",
    "- Use `StandardScaler` from `sklearn.preprocessing` for normalization of numeric features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:29:21.280692Z",
     "start_time": "2024-09-30T18:29:21.186714Z"
    },
    "id": "R7-4vnbIzp-o"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_housing_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "DZWGO7ewZiOn",
    "outputId": "b239ce79-58a2-4200-e345-45ad7b2f8662"
   },
   "outputs": [],
   "source": [
    "# Sample usage:\n",
    "# Load the dataset\n",
    "import pandas as pd\n",
    "file_path = '/data/IFI8410/sess10/HousePricePrediction.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Run the preprocessing function\n",
    "processed_df = preprocess_housing_data(df)\n",
    "\n",
    "# Display a few rows of the processed data\n",
    "processed_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8laWEoeZiOn"
   },
   "source": [
    "#### Save your solution to a file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:29:21.651471Z",
     "start_time": "2024-09-30T18:29:21.545780Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SajRmT5AZiOn",
    "outputId": "4c48be0c-9dfe-48a2-bcab-742f93c7c9de"
   },
   "outputs": [],
   "source": [
    "%%writefile solution_9_1.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_housing_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNknpBBrZiOn"
   },
   "source": [
    "#### Test 9.1 Execute the cell below to test your solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:29:21.951769Z",
     "start_time": "2024-09-30T18:29:21.720115Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YusNec-oZiOn",
    "outputId": "3a584ff1-c4f7-4676-e6d4-c1af224d029b"
   },
   "outputs": [],
   "source": [
    "! test/run_test.sh 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:29:22.022361Z",
     "start_time": "2024-09-30T18:29:22.019044Z"
    },
    "id": "vQl3ctAXZiOn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Qfed3KE2Wh9"
   },
   "source": [
    "### Problem 9.2: Feature Selection and Dataset Splitting\n",
    "\n",
    "**Question:**\n",
    "\n",
    "Using the preprocessed dataset from **Problem 9.1**, perform feature selection and split the dataset into training and testing sets.\n",
    "\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "- **Select Relevant Features**:\n",
    "\n",
    "    * Choose predictor columns that likely impact the house price based on your understanding of the dataset. \n",
    "    * For example, you may want to include features like `bedrooms, grade, living_in_m2, real_bathrooms`, and categorical indicators for conditions.\n",
    "    * Exclude the target column, price, from the features set.\n",
    "\n",
    "- **Split the Dataset**:\n",
    "\n",
    "    * Split the data into training and test sets with an 80:20 ratio.\n",
    "    * Use price as the target variable for prediction.\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "\n",
    "- You can use **price** as target column.\n",
    "  \n",
    "- Use `train_test_split` from `sklearn.model_selection` to divide the data into training and test sets.\n",
    "\n",
    "- Ensure that price is separated as the target variable, **y**, while other selected columns form **X**.\n",
    "\n",
    "- Return **X_train, X_test, y_train,** and **y_test**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:29:22.226699Z",
     "start_time": "2024-09-30T18:29:22.124362Z"
    },
    "id": "4cjZCE_Q2hN1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def select_and_split_data(df: pd.DataFrame, target_column: str = 'price') -> tuple:\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8_Sh6VFZiOo"
   },
   "outputs": [],
   "source": [
    "# Sample usage:\n",
    "# Load the dataset\n",
    "file_path = '/data/IFI8410/sess10/HousePricePrediction.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#processed_df` is the preprocessed DataFrame from Problem 9.1\n",
    "X_train, X_test, y_train, y_test = select_and_split_data(processed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlsMD8zXZiOo"
   },
   "source": [
    "#### Save your solution to a file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:29:22.551198Z",
     "start_time": "2024-09-30T18:29:22.451243Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3WoLmN_ZiOo",
    "outputId": "d1abd06e-a8b5-4f96-889b-520e3482a7d5"
   },
   "outputs": [],
   "source": [
    "%%writefile solution_9_2.py\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def select_and_split_data(df: pd.DataFrame, target_column: str = 'price') -> tuple:\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6Zay9KIZiOo"
   },
   "source": [
    "#### Test 9.2 Execute the cell below to test your solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:29:22.844763Z",
     "start_time": "2024-09-30T18:29:22.606809Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50Vqcn6oZiOo",
    "outputId": "9b1d7038-004f-4a60-ebfc-ec9091b422b1"
   },
   "outputs": [],
   "source": [
    "! test/run_test.sh 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AUT5bJTiKZW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UguNN8jDZiOo"
   },
   "source": [
    "### Problem 9.3: Model Training and Evaluation\n",
    "\n",
    "**Question:**\n",
    "\n",
    "Using the training and test sets from **Problem 9.2**, train a linear regression model to predict house prices and evaluate its performance.\n",
    "\n",
    "\n",
    "**Your task is to:**\n",
    "\n",
    "- **Train the Model**: Use LinearRegression from sklearn.linear_model to train a model on the training set.\n",
    "\n",
    "- **Evaluate the Model**: Calculate MAE, MSE, and RMSE on the test set predictions to understand the model’s performance.\n",
    "\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Use **LinearRegression()** to fit the model on the training data (X_train, y_train).\n",
    "\n",
    "- Predict house prices for X_test.\n",
    "\n",
    "- Calculate and return **MAE, MSE,** and **RMSE** metrics.\n",
    "\n",
    "\n",
    "**Hint**:\n",
    "\n",
    "- Use `mean_absolute_error, mean_squared_error, and np.sqrt()` to calculate evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTVwGNd_ZiOo"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test) -> tuple:\n",
    "    \n",
    "    # Write your code here\n",
    "\n",
    "\n",
    "\n",
    "    return mae, mse, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSGIa-OUZiOo",
    "outputId": "6152fc82-3420-42b4-941d-e433a5549a48"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Load the dataset\n",
    "file_path = '/data/IFI8410/sess10/HousePricePrediction.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined from Problem 9.2\n",
    "mae, mse, rmse = train_and_evaluate_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_In_ItuZiOo"
   },
   "source": [
    "#### Save your solution to a file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "faaGepf2ZiOp",
    "outputId": "688a54a7-8b81-4f3c-fd67-71c91447ae60"
   },
   "outputs": [],
   "source": [
    "%%writefile solution_9_3.py\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test) -> tuple:\n",
    "\n",
    "\n",
    "\n",
    "    return mae, mse, rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rjDbFLWZiOp"
   },
   "source": [
    "#### Test 9.3 Execute the cell below to test your solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBnZK_-EZiOp",
    "outputId": "4055c5d3-9398-43d7-a4e3-634692593a0b"
   },
   "outputs": [],
   "source": [
    "! test/run_test.sh 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZzh2YMDiKZX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPZu6cQpiKZX"
   },
   "source": [
    "### 9.4 Logistic Regression Problem: Predicting Employee Attrition\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "Train a Logistic Regression model using the employee dataset (**HR_Analytics.csv**) to predict whether an employee is likely to leave (attrition).\n",
    "\n",
    "\n",
    "**Your Task:**\n",
    "\n",
    "Write a function that performs the following steps:\n",
    "\n",
    "- `Data Loading and Preprocessing`:\n",
    "\n",
    "    * Load the dataset.Convert specified categorical columns into numerical format using One-Hot Encoding.\n",
    "\n",
    "    * Normalize numerical features to ensure consistency in model performance.\n",
    "\n",
    "- `Data Splitting`: Split the dataset into training and test sets, using 70% of the data for training and 30% for testing.\n",
    "\n",
    "- `Model Training`: Train a LogisticRegression model on the training set.\n",
    "\n",
    "- `Model Evaluation`: Evaluate the model's performance on the test set using the following metrics:\n",
    "\n",
    "    * Accuracy\n",
    "    \n",
    "    * Precision\n",
    "    \n",
    "    * Recall\n",
    "    \n",
    "    * F1-Score\n",
    "\n",
    "- Return these calculated metrics as function outputs.\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "\n",
    "- Encoding Categorical Features: Use OneHotEncoder to convert categorical columns to numerical values. Ensure unknown levels are ignored, and set the encoder to drop the first category of each feature.\n",
    "\n",
    "- Output Metrics: The function should return four metrics: **accuracy, precision, recall,** and **F1-score**.\n",
    "\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "- One-Hot Encoding: Use OneHotEncoder from sklearn on categorical columns with the settings drop='first' and handle_unknown='ignore' to avoid issues with new categories.\n",
    "\n",
    "- Data Splitting: Use a 70:30 train-test split ratio for balanced evaluation.\n",
    "Model Training: Use the LogisticRegression model from sklearn with appropriate settings.\n",
    "\n",
    "- Metrics Calculation: Calculate **accuracy, precision, recall,** and **F1-score** using `accuracy_score`, `precision_score`, `recall_score`, and `f1_score` `from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJRnlEBNZiOp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def process_and_train_logistic_regression(\n",
    "    df: pd.DataFrame, target: str, categorical_features: list, numeric_features: list\n",
    ") -> tuple:\n",
    "\n",
    "    # Write your code here\n",
    "    \n",
    "\n",
    "    return accuracy, precision, recall, f1score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5wWBWRBOiKZX",
    "outputId": "c95dfc42-8fc7-4f7b-e431-1d74c8a9cf44"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "file_path = '/data/IFI8410/sess10/HR_Analytics.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define target, categorical, and numeric columns\n",
    "target = 'left'\n",
    "categorical_features = ['Department', 'salary']\n",
    "numeric_features = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours',\n",
    "                    'time_spend_company', 'Work_accident', 'promotion_last_5years']\n",
    "\n",
    "# Run the function\n",
    "accuracy, precision, recall, f1_score = process_and_train_logistic_regression(\n",
    "    df, target, categorical_features, numeric_features\n",
    ")\n",
    "\n",
    "# Print labeled output\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uR5K1qq9iKZX"
   },
   "source": [
    "#### Save your solution to a file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vuPVQ0ciKZX",
    "outputId": "e391e543-21a1-4a44-cc56-3dffc1002d92"
   },
   "outputs": [],
   "source": [
    "%%writefile solution_9_4.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def process_and_train_logistic_regression(df: pd.DataFrame, target: str, categorical_features: list, numeric_features: list) -> tuple:\n",
    "\n",
    "\n",
    "    return accuracy, precision, recall, f1score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WQfm49WiKZX"
   },
   "source": [
    "#### Test 9.4 Execute the cell below to test your solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0oHM-9JiKZX",
    "outputId": "6095f1f4-52b8-4921-ce86-a3e2ecf2cd6c"
   },
   "outputs": [],
   "source": [
    "! test/run_test.sh 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "049thGnAiKZb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3F103HXZiOp"
   },
   "source": [
    "### 9.5: Visualizing Confusion Matrix for Employee Attrition Prediction\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "Extend the logistic regression model to visualize the confusion matrix, showing the number of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "Train the logistic regression model to predict employee attrition, as previously implemented.\n",
    "\n",
    "Visualize the model’s confusion matrix to better understand classification results.\n",
    "\n",
    "Your Task: Write a function that performs the following steps:\n",
    "\n",
    "\n",
    "**Model Training and Prediction:**\n",
    "\n",
    "Use the existing logistic regression model to train on the employee dataset and make predictions.\n",
    "\n",
    "\n",
    "**Confusion Matrix Calculation:**\n",
    "\n",
    "Calculate the confusion matrix metrics: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
    "\n",
    "\n",
    "**Visualization:**\n",
    "\n",
    "- Visualize the confusion matrix using a heatmap that clearly shows the counts for TP, TN, FP, and FN.\n",
    "\n",
    "- Add labels and color to the heatmap to improve interpretability.\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "\n",
    "- Use `confusion_matrix` from `sklearn.metrics` to calculate **TP, TN, FP,** and **FN**.\n",
    "  \n",
    "- Use `seaborn` and `matplotlib` to create a heatmap of the confusion matrix.\n",
    "\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "- Return the confusion matrix which is defined as `cm = confusion_matrix(y_test, y_pred)`.\n",
    "  \n",
    "- Visualize your results: Use `sns.heatmap` with annotations to visualize the confusion matrix. Label the axes as \"Predicted\" and \"Actual\" and add a title for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4bsfwWbZiOp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_confusion_matrix(df: pd.DataFrame, target: str, categorical_features: list, numeric_features: list) -> np.ndarray:\n",
    "    \n",
    "    # Write your code here\n",
    "\n",
    "\n",
    "\n",
    "    return cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "xxaqQ98RiKZb",
    "outputId": "4b826318-826d-4d87-c02d-b8f2bb3f1e80"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "file_path = '/data/IFI8410/sess10/HR_Analytics.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define target, categorical, and numeric columns\n",
    "target = 'left'\n",
    "categorical_features = ['Department', 'salary']\n",
    "numeric_features = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours',\n",
    "                    'time_spend_company', 'Work_accident', 'promotion_last_5years']\n",
    "\n",
    "# Run the function to visualize the confusion matrix\n",
    "cm = compute_confusion_matrix(df, target, categorical_features, numeric_features)\n",
    "\n",
    "# Visualize the confusion matrix using a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
    "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Employee Attrition Prediction')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0Ltf0x9iKZb"
   },
   "source": [
    "#### Save your solution to a file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqxRRIuKiKZb",
    "outputId": "f421c89d-0b0d-4ef2-b894-ade75cf25588"
   },
   "outputs": [],
   "source": [
    "%%writefile solution_9_5.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_confusion_matrix(df: pd.DataFrame, target: str, categorical_features: list, numeric_features: list) -> np.ndarray:\n",
    "    \n",
    "\n",
    "\n",
    "    return cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnWVfOb3ZiOp"
   },
   "source": [
    "#### Test 9.5 Execute the cell below to test your solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEFhdc_7dGff",
    "outputId": "9c7fb75a-51c4-44f6-f111-c332b22f1e2c"
   },
   "outputs": [],
   "source": [
    "! test/run_test.sh 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QOys_gciKZb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6 Load Data, Split in Train and Test Sets\n",
    "\n",
    "**Question:**\n",
    "\n",
    "You are given a CSV file **Fraud.csv** \n",
    "\n",
    "- Use a Naive Bayes Classifier to predict fraud based on categorical features from a given dataset. \n",
    "\n",
    "- Preprocess categorical features using one-hot encoding and use a pipeline to simplify the process of transforming and training the model.\n",
    "\n",
    "- Return the predicted labels as numpy array **y_pred_nb**.\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "- Use **Pipeline** to handle both preprocessing (one-hot encoding for categorical columns) and model fitting in one step.\n",
    "\n",
    "- **MultinomialNB** is suitable for categorical features, especially after one-hot encoding.\n",
    "\n",
    "- Set up the pipeline to include both the preprocessing of categorical data and fitting of the Naive Bayes model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def train_naive_bayes(file_path: str, categorical_features: list, target_feature: str) -> np.ndarray:\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "\n",
    "\n",
    "    return y_pred_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = '/data/IFI8410/sess10/Fraud_data.csv' \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and target\n",
    "categorical_features = [\n",
    "    'Make', 'AccidentArea', 'Sex', 'MaritalStatus', 'Fault', \n",
    "    'PolicyType', 'VehicleCategory', 'PoliceReportFiled', \n",
    "    'WitnessPresent', 'BasePolicy'\n",
    "]\n",
    "target_feature = 'FraudFound_P'\n",
    "\n",
    "y_pred_nb = train_naive_bayes(file_path, categorical_features, target_feature)\n",
    "\n",
    "print(\"Predictions:\", y_pred_nb[:10])  # Display first 10 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your solution to a file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile solution_9_6.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def train_naive_bayes(file_path: str, categorical_features: list, target_feature: str) -> np.ndarray:\n",
    "\n",
    "\n",
    "    \n",
    "    return y_pred_nb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 9.6 Execute the cell below to test your solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! test/run_test.sh 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7 Evaluation Metrics\n",
    "\n",
    "**Question:** \n",
    "\n",
    "Evaluate a **Naive Bayes model** on a test dataset using common classification metrics.\n",
    "\n",
    "After training your Naive Bayes model on a training set, evaluate the model on the test set by calculating the following metrics:\n",
    "\n",
    "- **Accuracy:** The overall proportion of correct predictions.\n",
    "\n",
    "- **Precision:** The proportion of true positive predictions out of all positive predictions.\n",
    "\n",
    "- **Recall:** The proportion of true positive predictions out of all actual positives.\n",
    "\n",
    "- **F1-Score:** The harmonic mean of precision and recall, providing a balance between the two.\n",
    "\n",
    "Return the final result in form of a dictionary:\n",
    "**{\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1_score\": f1\n",
    "}**\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "- You can use **accuracy_score**, **precision_score**, **recall_score**, and **f1_score** from `sklearn.metrics`.\n",
    "  \n",
    "- Ensure that **y_test** and **y_pred** are arrays of the same shape and data type.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxP0UZC5TGv0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def naive_bayes_metrics(file_path: str, categorical_features: list, target_feature: str) -> dict:\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Load data\n",
    "file_path = '/data/IFI8410/sess10/Fraud_data.csv' \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and target\n",
    "categorical_features = [\n",
    "    'Make', 'AccidentArea', 'Sex', 'MaritalStatus', 'Fault',\n",
    "    'PolicyType', 'VehicleCategory', 'PoliceReportFiled',\n",
    "    'WitnessPresent', 'BasePolicy'\n",
    "]\n",
    "target_feature = 'FraudFound_P'\n",
    "\n",
    "metrics = naive_bayes_metrics(file_path, categorical_features, target_feature)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {metrics['accuracy']:.2f}\")\n",
    "print(f\"Precision: {metrics['precision']:.2f}\")\n",
    "print(f\"Recall: {metrics['recall']:.2f}\")\n",
    "print(f\"F1-Score: {metrics['f1_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your solution to a file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bks29yrDTGkg"
   },
   "outputs": [],
   "source": [
    "%%writefile solution_9_7.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def naive_bayes_metrics(file_path: str, categorical_features: list, target_feature: str) -> dict:\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 9.7 Execute the cell below to test your solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! test/run_test.sh 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.8 Support Vector Machine (SVM)\n",
    "\n",
    "**Question:**\n",
    "\n",
    "You are provided with a dataset containing information about potential fraud cases. Your goal is to build an SVM model to predict fraud cases (**FraudFound_P** as target feature). However, the dataset is imbalanced, which could negatively impact the model's ability to detect fraud cases effectively. To improve performance:\n",
    "\n",
    "- Use `OneHotEncoder` to preprocess categorical features.\n",
    "- Apply `SMOTE` to balance the training data.\n",
    "- Train an **SVM classifier** with a non-linear kernel (RBF).\n",
    "- Return the predicted and original labels on the test set.\n",
    "\n",
    "\n",
    "**Task:** \n",
    "\n",
    "Write a function `load_and_train_svm_model` that:\n",
    "\n",
    "- Loads the dataset, processes categorical columns with `OneHotEncoder`, and applies `SMOTE` to balance the data.\n",
    "  \n",
    "- Trains the SVM model on the balanced data.\n",
    "  \n",
    "- Returns the predicted and original labels for the test data set **y_pred_svm, y_test**.\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "- Use `ColumnTransformer` to handle one-hot encoding of categorical features in a pipeline (see `scikit-learn` library).\n",
    "- Implement `SMOTE` from the `imblearn` library to create a balanced dataset by generating synthetic samples of the minority class. For example use `smote = SMOTE(random_state=1)` and then apply `smote.fit(.)` to your data.\n",
    "- Use SVC with the rbf kernel for non-linear classification.\n",
    "- Be sure to preprocess the test set using the same one-hot encoding pipeline before making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "def load_and_train_svm_model(file_path: str, categorical_features: list, target_feature: str):\n",
    "\n",
    "    # Write your code here\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    return y_pred_svm, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "file_path = '/data/IFI8410/sess10/Fraud_data.csv' \n",
    "categorical_features = [\n",
    "    'Make', 'AccidentArea', 'Sex', 'MaritalStatus', 'Fault', \n",
    "    'PolicyType', 'VehicleCategory', 'PoliceReportFiled', \n",
    "    'WitnessPresent', 'BasePolicy'\n",
    "]\n",
    "target_feature = 'FraudFound_P'\n",
    "\n",
    "y_pred_svm, y_test = load_and_train_svm_model(file_path, categorical_features, target_feature)\n",
    "print(\"SVM Predictions:\", y_pred_svm[:10]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your solution to a file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile solution_9_8.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "def load_and_train_svm_model(file_path: str, categorical_features: list, target_feature: str):\n",
    "\n",
    "    \n",
    "\n",
    "    return y_pred_svm, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 9.8 Execute the cell below to test your solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! test/run_test.sh 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.9 Evaluation Metrics\n",
    "\n",
    "**Question:**\n",
    "\n",
    "Given a dataset with a target class imbalance, train an **SVM classifier** to predict the target variable while handling the imbalance in the data. Use SMOTE to balance the training data and apply one-hot encoding to categorical features. Your task is to:\n",
    "\n",
    "**Task:** \n",
    "\n",
    "Write a function `train_and_evaluate_svm` with following tasks:\n",
    "\n",
    "- Load and preprocess the data, handling categorical features with one-hot encoding.\n",
    "- Balance the training dataset using `SMOTE`.\n",
    "- Train an SVM model using a pipeline that includes the necessary preprocessing and handles the class imbalance.\n",
    "- Evaluate the model on the test set using metrics such as **accuracy, precision, recall,** and **F1-score**.\n",
    "- Return the metrics as a dictyionary in the form \n",
    "  **metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred_svm),\n",
    "        \"precision\": precision_score(y_test, y_pred_svm, pos_label=1),\n",
    "        \"recall\": recall_score(y_test, y_pred_svm, pos_label=1),\n",
    "        \"f1_score\": f1_score(y_test, y_pred_svm, pos_label=1)\n",
    "    }**\n",
    "\n",
    "  \n",
    "**Hint:**\n",
    "\n",
    "- **One-Hot Encoding:** Use a `ColumnTransformer` to one-hot encode the categorical features, as this will help the model interpret the categorical data effectively.\n",
    "- **SMOTE for Re-balancing Target Data:** Apply SMOTE to the one-hot encoded training data to create a balanced dataset.\n",
    "- **Pipeline:** Set up a pipeline that includes preprocessing and model fitting. This will ensure that the same transformations are applied to both the training and test sets, maintaining consistency in feature representation.\n",
    "- **Class Weights in SVM:** Use class_weight='balanced' in the SVM model to handle any remaining imbalance, which helps optimize the decision boundary accordingly..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def train_and_evaluate_svm(file_path: str, categorical_features: list, target_feature: str) -> dict:\n",
    "\n",
    "    # Write your code here\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "file_path = '/data/IFI8410/sess10/Fraud_data.csv' \n",
    "categorical_features = [\n",
    "    'Make', 'AccidentArea', 'Sex', 'MaritalStatus', 'Fault', \n",
    "    'PolicyType', 'VehicleCategory', 'PoliceReportFiled', \n",
    "    'WitnessPresent', 'BasePolicy'\n",
    "]\n",
    "target_feature = 'FraudFound_P'\n",
    "\n",
    "metrics = train_and_evaluate_svm(file_path, categorical_features, target_feature)\n",
    "print(\"SVM Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your solution to a file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile solution_9_9.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def train_and_evaluate_svm(file_path: str, categorical_features: list, target_feature: str) -> dict:\n",
    "\n",
    "\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 9.9 Execute the cell below to test your solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! test/run_test.sh 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.10 Comparison of Performance for Naive Bayes and SVM\n",
    "\n",
    "**Question:**\n",
    "\n",
    "- Compare the performance of a **Naive Bayes classifier** and a **SVM classifier** on the given dataset. \n",
    "\n",
    "- Evaluate their respective **accuracy, precision, recall,** and **F1-score** to determine which model performs better. \n",
    "\n",
    "- Also, examine their confusion matrices to better understand where each model succeeds or fails.\n",
    "\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "Create a function `evaluate_model(y_true, y_pred, model_name)` that:\n",
    "\n",
    "- Prints all metrics. You can use this format:\n",
    "\n",
    "    `print(f\"\\n{model_name} Performance:\")`\n",
    "\n",
    "  \n",
    "    `print(f\"Accuracy: {accuracy:.2f}\")`\n",
    "\n",
    "  \n",
    "    `print(f\"Precision: {precision:.2f}\")`\n",
    "\n",
    "  \n",
    "    `print(f\"Recall: {recall:.2f}\")`\n",
    "\n",
    "  \n",
    "    `print(f\"F1-Score: {f1:.2f}\")`\n",
    "  \n",
    "- Plots the confusion matrix. (Note: You can use the `sklearn` class `ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)`.)\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "- Train Two Models: Use the Naive Bayes and SVM classifiers, applying the same preprocessing steps (like one-hot encoding for categorical features).\n",
    "\n",
    "- Evaluate with Metrics: After training both models, use accuracy, precision, recall, and F1-score to evaluate them.\n",
    "\n",
    "- Compare Confusion Matrices: Plot the confusion matrices to observe how each model performs on true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "- Interpret Results: Use the metrics and the confusion matrices to draw conclusions about the strengths and weaknesses of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load data\n",
    "file_path = '/data/IFI8410/sess10/Fraud_data.csv' \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and target\n",
    "categorical_features = [\n",
    "    'Make', 'AccidentArea', 'Sex', 'MaritalStatus', 'Fault', \n",
    "    'PolicyType', 'VehicleCategory', 'PoliceReportFiled', \n",
    "    'WitnessPresent', 'BasePolicy'\n",
    "]\n",
    "target_feature = 'FraudFound_P'\n",
    "\n",
    "# Split data into features and target\n",
    "X = df[categorical_features]\n",
    "y = df[target_feature]\n",
    "\n",
    "# Encode the target feature\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Set up preprocessing for categorical columns (One-Hot Encoding)\n",
    "preprocessorForCategoricalColumns = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "nbClassifier = MultinomialNB(alpha=1.0)\n",
    "nb_model = Pipeline(steps=[('preprocessor', preprocessorForCategoricalColumns), ('classifier', nbClassifier)])\n",
    "nb_model.fit(X_train, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "\n",
    "# SVM Classifier\n",
    "svmClassifier = SVC(kernel='linear', random_state=1)\n",
    "svm_model = Pipeline(steps=[('preprocessor', preprocessorForCategoricalColumns), ('classifier', svmClassifier)])\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \n",
    "    # Write your code here\n",
    "     \n",
    "\n",
    "\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "# Evaluate Naive Bayes\n",
    "evaluate_model(y_test, y_pred_nb, \"Naive Bayes Classifier\")\n",
    "\n",
    "# Evaluate SVM\n",
    "evaluate_model(y_test, y_pred_svm, \"SVM Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your solution to a file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile solution_9_10.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 9.10 Execute the cell below to test your solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! test/run_test.sh 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHlYhXH11dsJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHS2zsUfARtv"
   },
   "source": [
    "# Run all the tests again ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPxwUHEd1dul",
    "outputId": "711bd0ba-a354-4749-8cbc-1a2369e3d470"
   },
   "outputs": [],
   "source": [
    "! ./test/run_test.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXV85d2wAVx1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ml-HgeLbAYpq"
   },
   "source": [
    "# Homework Submission\n",
    "\n",
    "- This homework is due by **2024-11-13, 6:00 PM (EDT)**.\n",
    "\n",
    "- Make sure that all your programs and output files are in the exact folder as specified in the instructions.\n",
    "\n",
    "- All file names on this system are case sensitive. Verify if you copy your work from a local computer to your home directory on ARC.\n",
    "\n",
    "**Execute the cell below to submit your assignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwZl5dXMZiOp",
    "outputId": "2b39074e-7488-4143-a5a2-b0541420acd7"
   },
   "outputs": [],
   "source": [
    "! ./submit.sh -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKrOeGexZiOp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKcp_rsZZiOq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
