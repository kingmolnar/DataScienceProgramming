{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:05:42.618743",
     "start_time": "2017-08-20T22:05:42.605969"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "from scipy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Linear regression** is an approach for modeling the relationship between a scalar dependent variable $y$ and one or more explanatory variables (or independent variables) denoted $X$. The case of one explanatory variable is called *simple linear regression*. For more than one explanatory variable, the process is called *multiple linear regression*.$^1$ (This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.$^2$\n",
    "\n",
    "We assume that the equation\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$ where $\\epsilon_i \\approx N(0, \\sigma^2)$\n",
    "\n",
    "\n",
    "***\n",
    "$^1$ David A. Freedman (2009). Statistical Models: Theory and Practice. Cambridge University Press. p. 26. A simple regression equation has on the right hand side an intercept and an explanatory variable with a slope coefficient. A multiple regression equation has two or more explanatory variables on the right hand side, each with its own slope coefficient\n",
    "\n",
    "$^2$ Rencher, Alvin C.; Christensen, William F. (2012), \"Chapter 10, Multivariate regression – Section 10.1, Introduction\", Methods of Multivariate Analysis, Wiley Series in Probability and Statistics, 709 (3rd ed.), John Wiley &amp; Sons, p. 19, ISBN 9781118391679."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:05:46.837390",
     "start_time": "2017-08-20T22:05:46.652283"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n = 100  # numeber of samples\n",
    "Xr = np.random.rand(n)*99.0\n",
    "y = -7.3 + 2.5*Xr + np.random.randn(n)*27.0\n",
    "plt.plot(Xr, y, \"o\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-18T15:59:38.581412",
     "start_time": "2017-03-18T19:59:38.503Z"
    }
   },
   "source": [
    "Let's add the *bias*, i.e. a column of $1$s to the explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:05:49.789302",
     "start_time": "2017-08-20T22:05:49.777820"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.vstack((np.ones(n), Xr)).T\n",
    "print(X.shape)\n",
    "X[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed-form Linear Regression\n",
    "\n",
    "And compute the parametes $\\beta_0$  and $\\beta_1$ according to\n",
    "$$ \\beta = (X^\\prime X)^{-1} X^\\prime y $$\n",
    "***\n",
    "**Note:**\n",
    "This not only looks elegant but can also be written in Julia code. However, matrix inversion $M^{-1}$ requires $O(d^3)$ iterations for a $d\\times d$ matrix.<br /> \n",
    "https://www.coursera.org/learn/ml-regression/lecture/jOVX8/discussing-the-closed-form-solution"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-21T14:15:27.023844",
     "start_time": "2017-03-21T18:15:26.936Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Julia:  β = inv(X'*X)*X'*y    ## inv(M) for M⁻¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:05:54.218776",
     "start_time": "2017-08-20T22:05:54.212664"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:05:56.255917",
     "start_time": "2017-08-20T22:05:56.249354"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "yhat = X.dot(beta)\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:05:58.300169",
     "start_time": "2017-08-20T22:05:58.113274"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X[:,1], y, \"o\", alpha=0.5)\n",
    "plt.plot(X[:,1], yhat, \"-\", alpha=1, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-18T17:27:56.539013",
     "start_time": "2017-03-18T21:27:56.536Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T21:15:58.260896",
     "start_time": "2017-08-20T21:15:58.023952"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n = 100  # numeber of samples\n",
    "X1 = np.random.rand(n)*99.0\n",
    "X2 = np.random.rand(n)*51.0 - 26.8\n",
    "X3 = np.random.rand(n)*5.0 + 6.1\n",
    "X4 = np.random.rand(n)*1.0 - 0.5\n",
    "X5 = np.random.rand(n)*300.0\n",
    "\n",
    "y_m = -7.3 + 2.5*X1 + -7.9*X2 + 1.5*X3 + 10.0*X4 + 0.13*X5 + np.random.randn(n)*7.0\n",
    "plt.hist(y_m, bins=20)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T21:16:02.264716",
     "start_time": "2017-08-20T21:16:02.256656"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_m = np.vstack((np.ones(n), X1, X2, X3, X4, X5)).T\n",
    "X_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T21:16:08.245460",
     "start_time": "2017-08-20T21:16:08.236015"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### β = inv(X'*X)*X'*y\n",
    "beta_m = np.linalg.inv(X_m.T.dot(X_m)).dot(X_m.T).dot(y_m)\n",
    "beta_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T21:16:11.810061",
     "start_time": "2017-08-20T21:16:11.803105"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "yhat_m = X_m.dot(beta_m)\n",
    "yhat_m.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation: Root-mean-square Deviation\n",
    "The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample and population values) predicted by a model or an estimator and the values actually observed. The RMSD represents the sample standard deviation of the differences between predicted values and observed values. These individual differences are called residuals when the calculations are performed over the data sample that was used for estimation, and are called prediction errors when computed out-of-sample. The RMSD serves to aggregate the magnitudes of the errors in predictions for various times into a single measure of predictive power. RMSD is a good measure of accuracy, but only to compare forecasting errors of different models for a particular variable and not between variables, as it is scale-dependent.$^1$\n",
    "\n",
    "<!-- ![](imgs/fc187c3557d633423444d4c80a4a50cd6ecc3dd4.svg) -->\n",
    "***\n",
    "$^1$  Hyndman, Rob J. Koehler, Anne B.; Koehler (2006). \"Another look at measures of forecast accuracy\". International Journal of Forecasting. 22 (4): 679–688. doi:10.1016/j.ijforecast.2006.03.001."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-18T18:07:22.238294",
     "start_time": "2017-03-18T22:07:22.087Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "RSMD = √mean((ŷ-y).^2) \n",
    "## alternatively\n",
    "## Σ = sum\n",
    "## RSMD = √(Σ((ŷ-y).^2)/n) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T21:16:15.916471",
     "start_time": "2017-08-20T21:16:15.909747"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "RSMD = math.sqrt(np.square(yhat_m-y_m).sum()/n)\n",
    "print(RSMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization, Ridge-Regression\n",
    "\n",
    "Regularization, in mathematics and statistics and particularly in the fields of machine learning and inverse problems, is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting.\n",
    "\n",
    "\n",
    "In general, a regularization term $R(f)$ is introduced to a general loss function:\n",
    "\n",
    "\n",
    "\n",
    "for a loss function  $V$ that describes the cost of predicting $f(x)$ when the label is \n",
    "$y$, such as the square loss or hinge loss, and for the term \n",
    "$\\lambda$  which controls the importance of the regularization term. \n",
    "$R(f)$ is typically a penalty on the complexity of \n",
    "$f$, such as restrictions for smoothness or bounds on the vector space norm.$^1$\n",
    "\n",
    "A theoretical justification for regularization is that it attempts to impose Occam's razor on the solution, as depicted in the figure. From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters.\n",
    "\n",
    "Regularization can be used to learn simpler models, induce models to be sparse, introduce group structure into the learning problem, and more.\n",
    "\n",
    "We're going to add the L2 term $\\lambda||\\beta||_2^2$ to the regression equation, which yields to$^2$\n",
    "\n",
    "$$ \\beta = (X^\\prime X + \\lambda I)^{-1} X^\\prime y $$\n",
    "\n",
    "***\n",
    "$^1$ Bishop, Christopher M. (2007). Pattern recognition and machine learning (Corr. printing. ed.). New York: Springer. ISBN 978-0387310732.\n",
    "\n",
    "$^2$ http://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:07:29.976480",
     "start_time": "2017-08-20T22:07:29.968451"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "p = X.shape[1]  ## get number of parameters\n",
    "lam = 10.0\n",
    "p, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:07:42.548384",
     "start_time": "2017-08-20T22:07:42.543002"
    }
   },
   "outputs": [],
   "source": [
    "### β₂ = inv(X'*X + λ*eye(p))*X'*y\n",
    "beta2 = np.linalg.inv(X.T.dot(X) + lam*np.eye(p)).dot(X.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:07:44.953793",
     "start_time": "2017-08-20T22:07:44.949509"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "yhat2 = X.dot(beta2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:07:48.045581",
     "start_time": "2017-08-20T22:07:48.039255"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "RSMD₂ = √mean((ŷ₂-y).^2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:07:59.765819",
     "start_time": "2017-08-20T22:07:59.759320"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "RSMD2 = math.sqrt(np.square(yhat2-y).sum()/n)\n",
    "print(RSMD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:08:26.008985",
     "start_time": "2017-08-20T22:08:25.814418"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "##n = float(X.shape[0])\n",
    "print(f\"      RMSE = {math.sqrt(np.square(yhat-y).sum()/n)}\")\n",
    "print(f\"Ridge RMSE = {math.sqrt(np.square(yhat2-y).sum()/n)}\")\n",
    "plt.plot(X[:,1], y, \"o\", alpha=0.5)\n",
    "plt.plot(X[:,1], yhat, \"-\", alpha=0.7, color=\"red\")\n",
    "plt.plot(X[:,1], yhat2, \"-\", alpha=0.7, color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:08:56.517899",
     "start_time": "2017-08-20T22:08:56.509696"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "Xr_test = np.random.rand(n)*99.0\n",
    "X_test = np.vstack((np.ones(n), Xr_test)).T\n",
    "y_test = -7.3 + 2.5*Xr_test + np.random.randn(n)*27.0\n",
    "yhat_test = X_test.dot(beta)\n",
    "yhat_test2 = X_test.dot(beta2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:08:59.014233",
     "start_time": "2017-08-20T22:08:58.823704"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Test       RMSE = {math.sqrt(np.square(yhat_test-y_test).sum()/n)}\")\n",
    "print(f\"Test Ridge RMSE = {math.sqrt(np.square(yhat_test2-y_test).sum()/n)}\")\n",
    "plt.plot(X_test[:,1], y_test, \"o\", alpha=0.5)\n",
    "plt.plot(X_test[:,1], yhat_test, \"-\", alpha=1, color=\"red\")\n",
    "plt.plot(X_test[:,1], yhat_test2, \"-\", alpha=1, color=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run regressions with varying λ and σ\n",
    "Let's compute the (L2 regularized) linear regression for varying levels of noise, and regulization parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-21T09:53:36.881673",
     "start_time": "2017-03-21T13:53:36.646Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "rmse(ŷ, y) = √mean((ŷ-y).^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-21T10:02:04.054784",
     "start_time": "2017-03-21T14:02:03.927Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n = 100  # numeber of samples\n",
    "\n",
    "X1 = np.random.rand(n)*99.0\n",
    "X2 = np.random.rand(n)*51.0 - 26.8\n",
    "X3 = np.random.rand(n)*5.0 + 6.1\n",
    "X4 = np.random.rand(n)*1.0 - 0.5\n",
    "X5 = np.random.rand(n)*300.0\n",
    "X_tilde = np.random.rand(n)*1.0 - 0.5\n",
    "\n",
    "\n",
    "X = np.array([np.ones(n), X1, X2, X3, X4, X5])\n",
    "\n",
    "p = X.shape[1] ### size(X,2)\n",
    "\n",
    "for σ in [50, 100, 200]:\n",
    "    y = -7.3 + 2.5*X1 + -7.9*X2 + 1.5*X3 + 10.0*X4 + 0.13*X5 + 1.0*X_tilde**3 + σ*np.random.randn(n)\n",
    "   \n",
    "    for λ in [0.0, 2.0, 10.0, 100.0]:\n",
    "        # β = np.linalg.inv(X.T.dot(X) + λ * np.eye(p)).dot(X.T).dot(y)\n",
    "        # β = np.linalg.inv(X.T.dot(X) + λ * np.eye(p)).dot(X.T).dot(y)\n",
    "        β =np.linalg.inv(X.T.dot(X) + λ * np.eye(p)).dot(X.T).T.dot(y)\n",
    "        # =  np.linalg.inv(X'*X + λ*eye(p))*X'*y\n",
    "        # ŷ = X*β\n",
    "        y_hat = X.T.dot(β)\n",
    "        # R = rmse(y_hat, y)\n",
    "        R = math.sqrt(np.square(y_hat-y).sum()/p)\n",
    "        print(\"σ=%8.3lf\\tλ=%8.3lf\\tRMSD=%8.3lf\\n\" % ( σ, λ, R))\n",
    "        plt.plot(y, y_hat, '.', alpha=0.7)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(X.T.dot(X) + λ * np.eye(p)).dot(X.T).T.dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "In most cases we may rely on packages like scikit-learn to model data https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:16:13.257947",
     "start_time": "2017-08-20T22:16:13.058595"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Define input array with angles from 60deg to 300deg converted to radians\n",
    "Xr = np.array([i*np.pi/180 for i in range(60,300,4)])\n",
    "np.random.seed(10)  #Setting seed for reproducability\n",
    "y = np.sin(Xr) + np.random.normal(0,0.3,len(Xr))\n",
    "\n",
    "y_test = np.sin(Xr) + np.random.normal(0,0.3,len(Xr))\n",
    "plt.plot(Xr, y, 'o', alpha=0.5)\n",
    "plt.plot(Xr, y_test, 'v', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:17:06.142301",
     "start_time": "2017-08-20T22:17:06.128566"
    }
   },
   "outputs": [],
   "source": [
    "X = np.vstack((np.ones(len(Xr)), Xr)).T\n",
    "\n",
    "beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "yhat = X.dot(beta)\n",
    "yhat.shape\n",
    "\n",
    "p = X.shape[1]  ## get number of parameters\n",
    "lam = 10.0\n",
    "\n",
    "beta2 = np.linalg.inv(X.T.dot(X) + lam*np.eye(p)).dot(X.T).dot(y)\n",
    "yhat2 = X.dot(beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:17:09.909920",
     "start_time": "2017-08-20T22:17:09.713328"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n = float(X.shape[0])\n",
    "print(f\"      RMSE = {math.sqrt(np.square(yhat-y).sum()/n)}\")\n",
    "print(f\"Ridge RMSE = {math.sqrt(np.square(yhat2-y).sum()/n)}\")\n",
    "plt.plot(X[:,1], y, \"o\", alpha=0.5)\n",
    "plt.plot(X[:,1], yhat, \"-\", alpha=1, color=\"red\")\n",
    "plt.plot(X[:,1], yhat2, \"-\", alpha=1, color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T22:17:12.863135",
     "start_time": "2017-08-20T22:17:12.654009"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n = float(X.shape[0])\n",
    "print(f\"      RMSE = {math.sqrt(np.square(yhat-y_test).sum()/n)}\")\n",
    "print(f\"Ridge RMSE = {math.sqrt(np.square(yhat2-y_test).sum()/n)}\")\n",
    "plt.plot(X[:,1], y_test, \"o\", alpha=0.5)\n",
    "plt.plot(X[:,1], yhat, \"-\", alpha=1, color=\"red\")\n",
    "plt.plot(X[:,1], yhat2, \"-\", alpha=1, color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
